{
  "unit_tests": {
    "results": {
      "test_automl_controller.py": {
        "success": false,
        "returncode": 1,
        "stdout": "============================= test session starts =============================\nplatform win32 -- Python 3.13.2, pytest-8.4.1, pluggy-1.6.0 -- C:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python313\\python.exe\ncachedir: .pytest_cache\nrootdir: C:\\Users\\ADMIN\\Desktop\\MLTeammate\nconfigfile: pytest.ini\nplugins: anyio-4.9.0, cov-6.2.1\ncollecting ... collected 17 items\n\ntests/test_automl_controller.py::TestAutoMLController::test_controller_initialization PASSED [  5%]\ntests/test_automl_controller.py::TestAutoMLController::test_controller_initialization_defaults PASSED [ 11%]\ntests/test_automl_controller.py::TestAutoMLController::test_controller_invalid_task FAILED [ 17%]\ntests/test_automl_controller.py::TestAutoMLController::test_controller_invalid_n_trials FAILED [ 23%]\ntests/test_automl_controller.py::TestAutoMLController::test_controller_invalid_cv FAILED [ 29%]\ntests/test_automl_controller.py::TestAutoMLController::test_fit_classification FAILED [ 35%]\ntests/test_automl_controller.py::TestAutoMLController::test_fit_regression FAILED [ 41%]\ntests/test_automl_controller.py::TestAutoMLController::test_fit_without_cv FAILED [ 47%]\ntests/test_automl_controller.py::TestAutoMLController::test_predict FAILED [ 52%]\ntests/test_automl_controller.py::TestAutoMLController::test_score FAILED [ 58%]\ntests/test_automl_controller.py::TestAutoMLController::test_predict_without_fit FAILED [ 64%]\ntests/test_automl_controller.py::TestAutoMLController::test_score_without_fit FAILED [ 70%]\ntests/test_automl_controller.py::TestAutoMLController::test_callback_integration FAILED [ 76%]\ntests/test_automl_controller.py::TestAutoMLController::test_trial_failure_handling FAILED [ 82%]\ntests/test_automl_controller.py::TestAutoMLController::test_empty_data_handling FAILED [ 88%]\ntests/test_automl_controller.py::TestAutoMLController::test_single_sample_handling FAILED [ 94%]\ntests/test_automl_controller.py::TestAutoMLController::test_property_access FAILED [100%]\n\n================================== FAILURES ===================================\n______________ TestAutoMLController.test_controller_invalid_task ______________\n\nself = <test_automl_controller.TestAutoMLController object at 0x000002BFEB566650>\n\n    def test_controller_invalid_task(self):\n        \"\"\"Test controller initialization with invalid task.\"\"\"\n        searcher = OptunaSearcher(self.config_space)\n    \n>       with pytest.raises(ValueError, match=\"Task must be 'classification' or 'regression'\"):\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nE       Failed: DID NOT RAISE <class 'ValueError'>\n\ntests\\test_automl_controller.py:98: Failed\n---------------------------- Captured stderr call -----------------------------\n[I 2025-08-02 18:29:10,239] A new study created in memory with name: no-name-86823656-1d93-4c8d-9c8a-764c4247498e\n____________ TestAutoMLController.test_controller_invalid_n_trials ____________\n\nself = <test_automl_controller.TestAutoMLController object at 0x000002BFEB5668B0>\n\n    def test_controller_invalid_n_trials(self):\n        \"\"\"Test controller initialization with invalid n_trials.\"\"\"\n        searcher = OptunaSearcher(self.config_space)\n    \n>       with pytest.raises(ValueError, match=\"n_trials must be positive\"):\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nE       Failed: DID NOT RAISE <class 'ValueError'>\n\ntests\\test_automl_controller.py:110: Failed\n---------------------------- Captured stderr call -----------------------------\n[I 2025-08-02 18:29:10,928] A new study created in memory with name: no-name-9333a048-08fb-4960-acde-acca755b3375\n_______________ TestAutoMLController.test_controller_invalid_cv _______________\n\nself = <test_automl_controller.TestAutoMLController object at 0x000002BFEB640CB0>\n\n    def test_controller_invalid_cv(self):\n        \"\"\"Test controller initialization with invalid cv.\"\"\"\n        searcher = OptunaSearcher(self.config_space)\n    \n>       with pytest.raises(ValueError, match=\"cv must be positive\"):\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nE       Failed: DID NOT RAISE <class 'ValueError'>\n\ntests\\test_automl_controller.py:123: Failed\n---------------------------- Captured stderr call -----------------------------\n[I 2025-08-02 18:29:10,943] A new study created in memory with name: no-name-ede3d715-469c-40e2-b789-fd80d8f02dd0\n________________ TestAutoMLController.test_fit_classification _________________\n\nself = <test_automl_controller.TestAutoMLController object at 0x000002BFEB601590>\n\n    def test_fit_classification(self):\n        \"\"\"Test fitting the controller for classification task.\"\"\"\n        searcher = OptunaSearcher(self.config_space)\n        callbacks = [LoggerCallback(), ProgressCallback(total_trials=3)]\n    \n        controller = AutoMLController(\n            learners=self.mock_learners,\n            searcher=searcher,\n            config_space=self.config_space,\n            task=\"classification\",\n            n_trials=3,\n            cv=2,\n            callbacks=callbacks\n        )\n    \n        # Mock the searcher to return predictable results\n        with patch.object(searcher, 'suggest') as mock_suggest:\n            mock_suggest.return_value = {\n                \"learner_name\": \"random_forest\",\n                \"n_estimators\": 50,\n                \"max_depth\": 5\n            }\n    \n>           controller.fit(self.X_clf, self.y_clf)\n\ntests\\test_automl_controller.py:155: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\nml_teammate\\automl\\controller.py:86: in fit\n    cb.on_experiment_end(self.best_score, self.searcher.get_best())\n                                          ^^^^^^^^^^^^^^^^^^^^^^^^\nml_teammate\\search\\optuna_search.py:56: in get_best\n    return self.study.best_trial.params\n           ^^^^^^^^^^^^^^^^^^^^^\n..\\..\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\optuna\\study\\study.py:155: in best_trial\n    return self._get_best_trial(deepcopy=True)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n..\\..\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\optuna\\study\\study.py:307: in _get_best_trial\n    best_trial = self._storage.get_best_trial(self._study_id)\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nself = <optuna.storages._in_memory.InMemoryStorage object at 0x000002BFEB640050>\nstudy_id = 0\n\n    def get_best_trial(self, study_id: int) -> FrozenTrial:\n        with self._lock:\n            self._check_study_id(study_id)\n    \n            best_trial_id = self._studies[study_id].best_trial_id\n    \n            if best_trial_id is None:\n>               raise ValueError(\"No trials are completed yet.\")\nE               ValueError: No trials are completed yet.\n\n..\\..\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\optuna\\storages\\_in_memory.py:252: ValueError\n---------------------------- Captured stdout call -----------------------------\n[2025-08-02 18:29:10] [INFO] \\U0001f680 Starting MLTeammate experiment: mlteammate_experiment\\n[2025-08-02 18:29:10] [INFO] \\U0001f4cb Experiment config: {\\n  \"task\": \"classification\",\\n  \"n_trials\": 3,\\n  \"cv\": 2,\\n  \"learners\": [\\n    \"random_forest\",\\n    \"random_forest_regressor\"\\n  ],\\n  \"data_shape\": [\\n    100,\\n    10\\n  ]\\n}\\n\\U0001f3af Progress tracking enabled for 3 trials\\n[2025-08-02 18:29:10] [INFO] \\U0001f52c Starting trial 1 (ID: d0cb197d...)\\n[2025-08-02 18:29:10] [INFO] \\u2699\\ufe0f  Config: {\\n  \"learner_name\": \"random_forest\",\\n  \"n_estimators\": 50,\\n  \"max_depth\": 5\\n}\\n[Warning] Trial 1 failed: 'xgboost'\\n[2025-08-02 18:29:10] [INFO] \\U0001f52c Starting trial 2 (ID: df31b59f...)\\n[2025-08-02 18:29:10] [INFO] \\u2699\\ufe0f  Config: {\\n  \"learner_name\": \"random_forest\",\\n  \"n_estimators\": 50,\\n  \"max_depth\": 5\\n}\\n[Warning] Trial 2 failed: 'xgboost'\\n[2025-08-02 18:29:10] [INFO] \\U0001f52c Starting trial 3 (ID: 89293462...)\\n[2025-08-02 18:29:10] [INFO] \\u2699\\ufe0f  Config: {\\n  \"learner_name\": \"random_forest\",\\n  \"n_estimators\": 50,\\n  \"max_depth\": 5\\n}\\n[Warning] Trial 3 failed: 'xgboost'\n---------------------------- Captured stderr call -----------------------------\n[I 2025-08-02 18:29:10,956] A new study created in memory with name: no-name-5c36d04f-8795-48dc-beaa-69fb1f810590\n__________________ TestAutoMLController.test_fit_regression ___________________\n\nself = <test_automl_controller.TestAutoMLController object at 0x000002BFEB6017B0>\n\n    def test_fit_regression(self):\n        \"\"\"Test fitting the controller for regression task.\"\"\"\n        searcher = OptunaSearcher(self.config_space)\n        callbacks = [LoggerCallback(), ProgressCallback(total_trials=3)]\n    \n        controller = AutoMLController(\n            learners=self.mock_learners,\n            searcher=searcher,\n            config_space=self.config_space,\n            task=\"regression\",\n            n_trials=3,\n            cv=2,\n            callbacks=callbacks\n        )\n    \n        # Mock the searcher to return predictable results\n        with patch.object(searcher, 'suggest') as mock_suggest:\n            mock_suggest.return_value = {\n                \"learner_name\": \"random_forest_regressor\",\n                \"n_estimators\": 50,\n                \"max_depth\": 5\n            }\n    \n>           controller.fit(self.X_reg, self.y_reg)\n\ntests\\test_automl_controller.py:184: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\nml_teammate\\automl\\controller.py:86: in fit\n    cb.on_experiment_end(self.best_score, self.searcher.get_best())\n                                          ^^^^^^^^^^^^^^^^^^^^^^^^\nml_teammate\\search\\optuna_search.py:56: in get_best\n    return self.study.best_trial.params\n           ^^^^^^^^^^^^^^^^^^^^^\n..\\..\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\optuna\\study\\study.py:155: in best_trial\n    return self._get_best_trial(deepcopy=True)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n..\\..\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\optuna\\study\\study.py:307: in _get_best_trial\n    best_trial = self._storage.get_best_trial(self._study_id)\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nself = <optuna.storages._in_memory.InMemoryStorage object at 0x000002BFEB603570>\nstudy_id = 0\n\n    def get_best_trial(self, study_id: int) -> FrozenTrial:\n        with self._lock:\n            self._check_study_id(study_id)\n    \n            best_trial_id = self._studies[study_id].best_trial_id\n    \n            if best_trial_id is None:\n>               raise ValueError(\"No trials are completed yet.\")\nE               ValueError: No trials are completed yet.\n\n..\\..\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\optuna\\storages\\_in_memory.py:252: ValueError\n---------------------------- Captured stdout call -----------------------------\n[2025-08-02 18:29:11] [INFO] \\U0001f680 Starting MLTeammate experiment: mlteammate_experiment\\n[2025-08-02 18:29:11] [INFO] \\U0001f4cb Experiment config: {\\n  \"task\": \"regression\",\\n  \"n_trials\": 3,\\n  \"cv\": 2,\\n  \"learners\": [\\n    \"random_forest\",\\n    \"random_forest_regressor\"\\n  ],\\n  \"data_shape\": [\\n    100,\\n    10\\n  ]\\n}\\n\\U0001f3af Progress tracking enabled for 3 trials\\n[2025-08-02 18:29:11] [INFO] \\U0001f52c Starting trial 1 (ID: 8b13155e...)\\n[2025-08-02 18:29:11] [INFO] \\u2699\\ufe0f  Config: {\\n  \"learner_name\": \"random_forest_regressor\",\\n  \"n_estimators\": 50,\\n  \"max_depth\": 5\\n}\\n[Warning] Trial 1 failed: 'xgboost'\\n[2025-08-02 18:29:11] [INFO] \\U0001f52c Starting trial 2 (ID: 563dfec1...)\\n[2025-08-02 18:29:11] [INFO] \\u2699\\ufe0f  Config: {\\n  \"learner_name\": \"random_forest_regressor\",\\n  \"n_estimators\": 50,\\n  \"max_depth\": 5\\n}\\n[Warning] Trial 2 failed: 'xgboost'\\n[2025-08-02 18:29:11] [INFO] \\U0001f52c Starting trial 3 (ID: fb7ecd06...)\\n[2025-08-02 18:29:11] [INFO] \\u2699\\ufe0f  Config: {\\n  \"learner_name\": \"random_forest_regressor\",\\n  \"n_estimators\": 50,\\n  \"max_depth\": 5\\n}\\n[Warning] Trial 3 failed: 'xgboost'\n---------------------------- Captured stderr call -----------------------------\n[I 2025-08-02 18:29:11,034] A new study created in memory with name: no-name-4979ce1b-18a0-4aca-a4f6-71d22e1ada62\n__________________ TestAutoMLController.test_fit_without_cv ___________________\n\nself = <test_automl_controller.TestAutoMLController object at 0x000002BFEB57AD50>\n\n    def test_fit_without_cv(self):\n        \"\"\"Test fitting without cross-validation.\"\"\"\n        searcher = OptunaSearcher(self.config_space)\n    \n        controller = AutoMLController(\n            learners=self.mock_learners,\n            searcher=searcher,\n            config_space=self.config_space,\n            task=\"classification\",\n            n_trials=3,\n            cv=None,\n            callbacks=[]\n        )\n    \n        # Mock the searcher\n        with patch.object(searcher, 'suggest') as mock_suggest:\n            mock_suggest.return_value = {\n                \"learner_name\": \"random_forest\",\n                \"n_estimators\": 50,\n                \"max_depth\": 5\n            }\n    \n            controller.fit(self.X_clf, self.y_clf)\n    \n>       assert controller.best_score is not None\nE       assert None is not None\nE        +  where None = <ml_teammate.automl.controller.AutoMLController object at 0x000002BFED9D4D10>.best_score\n\ntests\\test_automl_controller.py:214: AssertionError\n---------------------------- Captured stdout call -----------------------------\n[Warning] Trial 1 failed: 'xgboost'\n[Warning] Trial 2 failed: 'xgboost'\n[Warning] Trial 3 failed: 'xgboost'\n---------------------------- Captured stderr call -----------------------------\n[I 2025-08-02 18:29:11,092] A new study created in memory with name: no-name-a1a06ccc-1591-44ab-a78e-c51861dd81fe\n______________________ TestAutoMLController.test_predict ______________________\n\nself = <test_automl_controller.TestAutoMLController object at 0x000002BFEB57AF50>\n\n    def test_predict(self):\n        \"\"\"Test making predictions.\"\"\"\n        searcher = OptunaSearcher(self.config_space)\n    \n        controller = AutoMLController(\n            learners=self.mock_learners,\n            searcher=searcher,\n            config_space=self.config_space,\n            task=\"classification\",\n            n_trials=3,\n            cv=2\n        )\n    \n        # Mock the searcher and fit\n        with patch.object(searcher, 'suggest') as mock_suggest:\n            mock_suggest.return_value = {\n                \"learner_name\": \"random_forest\",\n                \"n_estimators\": 50,\n                \"max_depth\": 5\n            }\n    \n            controller.fit(self.X_clf, self.y_clf)\n    \n        # Test predictions\n        X_test = self.X_clf[:10]\n>       predictions = controller.predict(X_test)\n                      ^^^^^^^^^^^^^^^^^^^^^^^^^^\n\ntests\\test_automl_controller.py:241: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nself = <ml_teammate.automl.controller.AutoMLController object at 0x000002BFED9B5450>\nX = array([[-1.14052601,  1.35970566,  0.86199147,  0.84609208,  0.60600995,\n        -1.55662917,  1.75479418,  1.69645637...9353,  0.55942643,  1.7240021 , -0.38455554,\n        -0.86041337, -0.57689187, -1.12970685,  1.00629281,  0.83569211]])\n\n    def predict(self, X):\n        if self.best_model is None:\n>           raise ValueError(\"No trained model found. Please call .fit() first and ensure at least one successful trial.\")\nE           ValueError: No trained model found. Please call .fit() first and ensure at least one successful trial.\n\nml_teammate\\automl\\controller.py:90: ValueError\n---------------------------- Captured stdout call -----------------------------\n[Warning] Trial 1 failed: 'xgboost'\n[Warning] Trial 2 failed: 'xgboost'\n[Warning] Trial 3 failed: 'xgboost'\n---------------------------- Captured stderr call -----------------------------\n[I 2025-08-02 18:29:11,106] A new study created in memory with name: no-name-c4d6b6e0-d92e-42b3-9e1d-631041302d30\n_______________________ TestAutoMLController.test_score _______________________\n\nself = <test_automl_controller.TestAutoMLController object at 0x000002BFEB6347D0>\n\n    def test_score(self):\n        \"\"\"Test scoring the model.\"\"\"\n        searcher = OptunaSearcher(self.config_space)\n    \n        controller = AutoMLController(\n            learners=self.mock_learners,\n            searcher=searcher,\n            config_space=self.config_space,\n            task=\"classification\",\n            n_trials=3,\n            cv=2\n        )\n    \n        # Mock the searcher and fit\n        with patch.object(searcher, 'suggest') as mock_suggest:\n            mock_suggest.return_value = {\n                \"learner_name\": \"random_forest\",\n                \"n_estimators\": 50,\n                \"max_depth\": 5\n            }\n    \n            controller.fit(self.X_clf, self.y_clf)\n    \n        # Test scoring\n        X_test, y_test = self.X_clf[:20], self.y_clf[:20]\n>       score = controller.score(X_test, y_test)\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\ntests\\test_automl_controller.py:271: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\nml_teammate\\automl\\controller.py:98: in score\n    preds = self.predict(X)\n            ^^^^^^^^^^^^^^^\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nself = <ml_teammate.automl.controller.AutoMLController object at 0x000002BFED9B6550>\nX = array([[-1.14052601,  1.35970566,  0.86199147,  0.84609208,  0.60600995,\n        -1.55662917,  1.75479418,  1.69645637...1892,  0.93343952,  0.30446853, -0.77781669,\n        -1.40751169,  1.75227044,  1.27155509, -1.11057585,  0.93567839]])\n\n    def predict(self, X):\n        if self.best_model is None:\n>           raise ValueError(\"No trained model found. Please call .fit() first and ensure at least one successful trial.\")\nE           ValueError: No trained model found. Please call .fit() first and ensure at least one successful trial.\n\nml_teammate\\automl\\controller.py:90: ValueError\n---------------------------- Captured stdout call -----------------------------\n[Warning] Trial 1 failed: 'xgboost'\n[Warning] Trial 2 failed: 'xgboost'\n[Warning] Trial 3 failed: 'xgboost'\n---------------------------- Captured stderr call -----------------------------\n[I 2025-08-02 18:29:11,123] A new study created in memory with name: no-name-004c5eb0-4e8a-413f-8d8e-ef6d9945b1a4\n________________ TestAutoMLController.test_predict_without_fit ________________\n\nself = <test_automl_controller.TestAutoMLController object at 0x000002BFEB6349B0>\n\n    def test_predict_without_fit(self):\n        \"\"\"Test that predict raises error if model not fitted.\"\"\"\n        searcher = OptunaSearcher(self.config_space)\n    \n        controller = AutoMLController(\n            learners=self.mock_learners,\n            searcher=searcher,\n            config_space=self.config_space,\n            task=\"classification\",\n            n_trials=3,\n            cv=2\n        )\n    \n        with pytest.raises(ValueError, match=\"Model not fitted\"):\n>           controller.predict(self.X_clf[:10])\n\ntests\\test_automl_controller.py:290: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nself = <ml_teammate.automl.controller.AutoMLController object at 0x000002BFED764B90>\nX = array([[-1.14052601,  1.35970566,  0.86199147,  0.84609208,  0.60600995,\n        -1.55662917,  1.75479418,  1.69645637...9353,  0.55942643,  1.7240021 , -0.38455554,\n        -0.86041337, -0.57689187, -1.12970685,  1.00629281,  0.83569211]])\n\n    def predict(self, X):\n        if self.best_model is None:\n>           raise ValueError(\"No trained model found. Please call .fit() first and ensure at least one successful trial.\")\nE           ValueError: No trained model found. Please call .fit() first and ensure at least one successful trial.\n\nml_teammate\\automl\\controller.py:90: ValueError\n\nDuring handling of the above exception, another exception occurred:\n\nself = <test_automl_controller.TestAutoMLController object at 0x000002BFEB6349B0>\n\n    def test_predict_without_fit(self):\n        \"\"\"Test that predict raises error if model not fitted.\"\"\"\n        searcher = OptunaSearcher(self.config_space)\n    \n        controller = AutoMLController(\n            learners=self.mock_learners,\n            searcher=searcher,\n            config_space=self.config_space,\n            task=\"classification\",\n            n_trials=3,\n            cv=2\n        )\n    \n>       with pytest.raises(ValueError, match=\"Model not fitted\"):\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nE       AssertionError: Regex pattern did not match.\nE        Regex: 'Model not fitted'\nE        Input: 'No trained model found. Please call .fit() first and ensure at least one successful trial.'\n\ntests\\test_automl_controller.py:289: AssertionError\n---------------------------- Captured stderr call -----------------------------\n[I 2025-08-02 18:29:11,141] A new study created in memory with name: no-name-ae37a531-96be-4798-b478-49fc06254cbc\n_________________ TestAutoMLController.test_score_without_fit _________________\n\nself = <test_automl_controller.TestAutoMLController object at 0x000002BFEB63DE10>\n\n    def test_score_without_fit(self):\n        \"\"\"Test that score raises error if model not fitted.\"\"\"\n        searcher = OptunaSearcher(self.config_space)\n    \n        controller = AutoMLController(\n            learners=self.mock_learners,\n            searcher=searcher,\n            config_space=self.config_space,\n            task=\"classification\",\n            n_trials=3,\n            cv=2\n        )\n    \n        with pytest.raises(ValueError, match=\"Model not fitted\"):\n>           controller.score(self.X_clf[:10], self.y_clf[:10])\n\ntests\\test_automl_controller.py:306: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\nml_teammate\\automl\\controller.py:98: in score\n    preds = self.predict(X)\n            ^^^^^^^^^^^^^^^\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nself = <ml_teammate.automl.controller.AutoMLController object at 0x000002BFEB637020>\nX = array([[-1.14052601,  1.35970566,  0.86199147,  0.84609208,  0.60600995,\n        -1.55662917,  1.75479418,  1.69645637...9353,  0.55942643,  1.7240021 , -0.38455554,\n        -0.86041337, -0.57689187, -1.12970685,  1.00629281,  0.83569211]])\n\n    def predict(self, X):\n        if self.best_model is None:\n>           raise ValueError(\"No trained model found. Please call .fit() first and ensure at least one successful trial.\")\nE           ValueError: No trained model found. Please call .fit() first and ensure at least one successful trial.\n\nml_teammate\\automl\\controller.py:90: ValueError\n\nDuring handling of the above exception, another exception occurred:\n\nself = <test_automl_controller.TestAutoMLController object at 0x000002BFEB63DE10>\n\n    def test_score_without_fit(self):\n        \"\"\"Test that score raises error if model not fitted.\"\"\"\n        searcher = OptunaSearcher(self.config_space)\n    \n        controller = AutoMLController(\n            learners=self.mock_learners,\n            searcher=searcher,\n            config_space=self.config_space,\n            task=\"classification\",\n            n_trials=3,\n            cv=2\n        )\n    \n>       with pytest.raises(ValueError, match=\"Model not fitted\"):\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nE       AssertionError: Regex pattern did not match.\nE        Regex: 'Model not fitted'\nE        Input: 'No trained model found. Please call .fit() first and ensure at least one successful trial.'\n\ntests\\test_automl_controller.py:305: AssertionError\n---------------------------- Captured stderr call -----------------------------\n[I 2025-08-02 18:29:11,162] A new study created in memory with name: no-name-db292e8e-782c-433d-993c-4a29269d08e9\n_______________ TestAutoMLController.test_callback_integration ________________\n\nself = <test_automl_controller.TestAutoMLController object at 0x000002BFEB63E270>\n\n    def test_callback_integration(self):\n        \"\"\"Test that callbacks are called during fitting.\"\"\"\n        searcher = OptunaSearcher(self.config_space)\n    \n        # Create mock callbacks\n        mock_callback = Mock()\n        callbacks = [mock_callback]\n    \n        controller = AutoMLController(\n            learners=self.mock_learners,\n            searcher=searcher,\n            config_space=self.config_space,\n            task=\"classification\",\n            n_trials=3,\n            cv=2,\n            callbacks=callbacks\n        )\n    \n        # Mock the searcher\n        with patch.object(searcher, 'suggest') as mock_suggest:\n            mock_suggest.return_value = {\n                \"learner_name\": \"random_forest\",\n                \"n_estimators\": 50,\n                \"max_depth\": 5\n            }\n    \n>           controller.fit(self.X_clf, self.y_clf)\n\ntests\\test_automl_controller.py:334: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\nml_teammate\\automl\\controller.py:86: in fit\n    cb.on_experiment_end(self.best_score, self.searcher.get_best())\n                                          ^^^^^^^^^^^^^^^^^^^^^^^^\nml_teammate\\search\\optuna_search.py:56: in get_best\n    return self.study.best_trial.params\n           ^^^^^^^^^^^^^^^^^^^^^\n..\\..\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\optuna\\study\\study.py:155: in best_trial\n    return self._get_best_trial(deepcopy=True)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n..\\..\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\optuna\\study\\study.py:307: in _get_best_trial\n    best_trial = self._storage.get_best_trial(self._study_id)\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nself = <optuna.storages._in_memory.InMemoryStorage object at 0x000002BFC510C050>\nstudy_id = 0\n\n    def get_best_trial(self, study_id: int) -> FrozenTrial:\n        with self._lock:\n            self._check_study_id(study_id)\n    \n            best_trial_id = self._studies[study_id].best_trial_id\n    \n            if best_trial_id is None:\n>               raise ValueError(\"No trials are completed yet.\")\nE               ValueError: No trials are completed yet.\n\n..\\..\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\optuna\\storages\\_in_memory.py:252: ValueError\n---------------------------- Captured stdout call -----------------------------\n[Warning] Trial 1 failed: 'xgboost'\n[Warning] Trial 2 failed: 'xgboost'\n[Warning] Trial 3 failed: 'xgboost'\n---------------------------- Captured stderr call -----------------------------\n[I 2025-08-02 18:29:11,325] A new study created in memory with name: no-name-5b972f68-e07c-45ba-8e12-0c3e6da5edc1\n______________ TestAutoMLController.test_trial_failure_handling _______________\n\nself = <test_automl_controller.TestAutoMLController object at 0x000002BFEB5C3520>\n\n    def test_trial_failure_handling(self):\n        \"\"\"Test handling of trial failures.\"\"\"\n        searcher = OptunaSearcher(self.config_space)\n    \n        # Create a learner that raises an exception\n        failing_learners = {\n            \"failing_learner\": Mock(side_effect=Exception(\"Learner failed\"))\n        }\n    \n        controller = AutoMLController(\n            learners=failing_learners,\n            searcher=searcher,\n            config_space=self.config_space,\n            task=\"classification\",\n            n_trials=3,\n            cv=2\n        )\n    \n        # Mock the searcher\n        with patch.object(searcher, 'suggest') as mock_suggest:\n            mock_suggest.return_value = {\n                \"learner_name\": \"failing_learner\",\n                \"n_estimators\": 50,\n                \"max_depth\": 5\n            }\n    \n            # Should not raise exception, should handle gracefully\n            controller.fit(self.X_clf, self.y_clf)\n    \n        # Should still have some results even with failures\n>       assert controller.best_score is not None\nE       assert None is not None\nE        +  where None = <ml_teammate.automl.controller.AutoMLController object at 0x000002BFED791A90>.best_score\n\ntests\\test_automl_controller.py:370: AssertionError\n---------------------------- Captured stdout call -----------------------------\n[Warning] Trial 1 failed: 'xgboost'\n[Warning] Trial 2 failed: 'xgboost'\n[Warning] Trial 3 failed: 'xgboost'\n---------------------------- Captured stderr call -----------------------------\n[I 2025-08-02 18:29:11,375] A new study created in memory with name: no-name-ad4152f1-b464-4b61-8b40-7197581c5b7f\n________________ TestAutoMLController.test_empty_data_handling ________________\n\nself = <test_automl_controller.TestAutoMLController object at 0x000002BFEB6481D0>\n\n    def test_empty_data_handling(self):\n        \"\"\"Test handling of empty datasets.\"\"\"\n        searcher = OptunaSearcher(self.config_space)\n    \n        controller = AutoMLController(\n            learners=self.mock_learners,\n            searcher=searcher,\n            config_space=self.config_space,\n            task=\"classification\",\n            n_trials=3,\n            cv=2\n        )\n    \n        # Test with empty data\n>       with pytest.raises(ValueError, match=\"Empty dataset\"):\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nE       Failed: DID NOT RAISE <class 'ValueError'>\n\ntests\\test_automl_controller.py:386: Failed\n---------------------------- Captured stdout call -----------------------------\n[Warning] Trial 1 failed: 'xgboost'\n[Warning] Trial 2 failed: 'xgboost'\n[Warning] Trial 3 failed: 'xgboost'\n---------------------------- Captured stderr call -----------------------------\n[I 2025-08-02 18:29:11,389] A new study created in memory with name: no-name-f290559e-c939-4c8b-a17e-67f3b619a91a\n______________ TestAutoMLController.test_single_sample_handling _______________\n\nself = <test_automl_controller.TestAutoMLController object at 0x000002BFEB648350>\n\n    def test_single_sample_handling(self):\n        \"\"\"Test handling of single sample datasets.\"\"\"\n        searcher = OptunaSearcher(self.config_space)\n    \n        controller = AutoMLController(\n            learners=self.mock_learners,\n            searcher=searcher,\n            config_space=self.config_space,\n            task=\"classification\",\n            n_trials=3,\n            cv=2\n        )\n    \n        # Test with single sample\n        X_single = self.X_clf[:1]\n        y_single = self.y_clf[:1]\n    \n>       with pytest.raises(ValueError, match=\"Insufficient samples\"):\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nE       Failed: DID NOT RAISE <class 'ValueError'>\n\ntests\\test_automl_controller.py:406: Failed\n---------------------------- Captured stdout call -----------------------------\n[Warning] Trial 1 failed: 'xgboost'\n[Warning] Trial 2 failed: 'xgboost'\n[Warning] Trial 3 failed: 'xgboost'\n---------------------------- Captured stderr call -----------------------------\n[I 2025-08-02 18:29:11,403] A new study created in memory with name: no-name-aea8c212-f932-420e-bc94-0f0885e1ff15\n__________________ TestAutoMLController.test_property_access __________________\n\nself = <test_automl_controller.TestAutoMLController object at 0x000002BFEB5E1B20>\n\n    def test_property_access(self):\n        \"\"\"Test accessing controller properties.\"\"\"\n        searcher = OptunaSearcher(self.config_space)\n    \n        controller = AutoMLController(\n            learners=self.mock_learners,\n            searcher=searcher,\n            config_space=self.config_space,\n            task=\"classification\",\n            n_trials=3,\n            cv=2\n        )\n    \n        # Test properties before fitting\n        assert controller.best_score is None\n        assert controller.best_model is None\n>       assert controller.best_config is None\n               ^^^^^^^^^^^^^^^^^^^^^^\nE       AttributeError: 'AutoMLController' object has no attribute 'best_config'\n\ntests\\test_automl_controller.py:425: AttributeError\n---------------------------- Captured stderr call -----------------------------\n[I 2025-08-02 18:29:11,416] A new study created in memory with name: no-name-4651a3ea-e6f5-46f6-8b4a-543b518f308d\n============================== warnings summary ===============================\n..\\..\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pydantic\\_internal\\_config.py:323\n  C:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pydantic\\_internal\\_config.py:323: PydanticDeprecatedSince20: Support for class-based `config` is deprecated, use ConfigDict instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/\n    warnings.warn(DEPRECATION_MESSAGE, DeprecationWarning)\n\n-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html\n=========================== short test summary info ===========================\nFAILED tests/test_automl_controller.py::TestAutoMLController::test_controller_invalid_task\nFAILED tests/test_automl_controller.py::TestAutoMLController::test_controller_invalid_n_trials\nFAILED tests/test_automl_controller.py::TestAutoMLController::test_controller_invalid_cv\nFAILED tests/test_automl_controller.py::TestAutoMLController::test_fit_classification\nFAILED tests/test_automl_controller.py::TestAutoMLController::test_fit_regression\nFAILED tests/test_automl_controller.py::TestAutoMLController::test_fit_without_cv\nFAILED tests/test_automl_controller.py::TestAutoMLController::test_predict - ...\nFAILED tests/test_automl_controller.py::TestAutoMLController::test_score - Va...\nFAILED tests/test_automl_controller.py::TestAutoMLController::test_predict_without_fit\nFAILED tests/test_automl_controller.py::TestAutoMLController::test_score_without_fit\nFAILED tests/test_automl_controller.py::TestAutoMLController::test_callback_integration\nFAILED tests/test_automl_controller.py::TestAutoMLController::test_trial_failure_handling\nFAILED tests/test_automl_controller.py::TestAutoMLController::test_empty_data_handling\nFAILED tests/test_automl_controller.py::TestAutoMLController::test_single_sample_handling\nFAILED tests/test_automl_controller.py::TestAutoMLController::test_property_access\n================== 15 failed, 2 passed, 1 warning in 10.03s ===================\n",
        "stderr": ""
      },
      "test_search_components.py": {
        "success": false,
        "returncode": 1,
        "stdout": "============================= test session starts =============================\nplatform win32 -- Python 3.13.2, pytest-8.4.1, pluggy-1.6.0 -- C:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python313\\python.exe\ncachedir: .pytest_cache\nrootdir: C:\\Users\\ADMIN\\Desktop\\MLTeammate\nconfigfile: pytest.ini\nplugins: anyio-4.9.0, cov-6.2.1\ncollecting ... collected 29 items\n\ntests/test_search_components.py::TestOptunaSearcher::test_optuna_searcher_initialization FAILED [  3%]\ntests/test_search_components.py::TestOptunaSearcher::test_optuna_searcher_with_custom_study FAILED [  6%]\ntests/test_search_components.py::TestOptunaSearcher::test_suggest_parameters FAILED [ 10%]\ntests/test_search_components.py::TestOptunaSearcher::test_suggest_parameters_different_learner FAILED [ 13%]\ntests/test_search_components.py::TestOptunaSearcher::test_report_results FAILED [ 17%]\ntests/test_search_components.py::TestOptunaSearcher::test_get_best_results FAILED [ 20%]\ntests/test_search_components.py::TestOptunaSearcher::test_invalid_learner_name FAILED [ 24%]\ntests/test_search_components.py::TestOptunaSearcher::test_empty_config_space FAILED [ 27%]\ntests/test_search_components.py::TestFLAMLSearcher::test_flaml_searcher_initialization PASSED [ 31%]\ntests/test_search_components.py::TestFLAMLSearcher::test_flaml_searcher_custom_settings PASSED [ 34%]\ntests/test_search_components.py::TestFLAMLSearcher::test_convert_config_space FAILED [ 37%]\ntests/test_search_components.py::TestFLAMLSearcher::test_flaml_fit FAILED [ 41%]\ntests/test_search_components.py::TestFLAMLSearcher::test_get_best_results FAILED [ 44%]\ntests/test_search_components.py::TestEarlyConvergenceIndicator::test_eci_initialization FAILED [ 48%]\ntests/test_search_components.py::TestEarlyConvergenceIndicator::test_add_trial FAILED [ 51%]\ntests/test_search_components.py::TestEarlyConvergenceIndicator::test_moving_average_convergence PASSED [ 55%]\ntests/test_search_components.py::TestEarlyConvergenceIndicator::test_improvement_rate_convergence FAILED [ 58%]\ntests/test_search_components.py::TestEarlyConvergenceIndicator::test_confidence_interval_convergence PASSED [ 62%]\ntests/test_search_components.py::TestEarlyConvergenceIndicator::test_plateau_convergence PASSED [ 65%]\ntests/test_search_components.py::TestEarlyConvergenceIndicator::test_get_convergence_info FAILED [ 68%]\ntests/test_search_components.py::TestEarlyConvergenceIndicator::test_reset FAILED [ 72%]\ntests/test_search_components.py::TestAdaptiveECI::test_adaptive_eci_initialization PASSED [ 75%]\ntests/test_search_components.py::TestAdaptiveECI::test_adaptive_parameter_adjustment PASSED [ 79%]\ntests/test_search_components.py::TestMultiObjectiveECI::test_multi_objective_eci_initialization FAILED [ 82%]\ntests/test_search_components.py::TestMultiObjectiveECI::test_multi_objective_trial_addition FAILED [ 86%]\ntests/test_search_components.py::TestSearchFactoryFunctions::test_get_searcher PASSED [ 89%]\ntests/test_search_components.py::TestSearchFactoryFunctions::test_get_eci PASSED [ 93%]\ntests/test_search_components.py::TestSearchFactoryFunctions::test_list_available_searchers FAILED [ 96%]\ntests/test_search_components.py::TestSearchFactoryFunctions::test_list_available_eci_types PASSED [100%]\n\n================================== FAILURES ===================================\n___________ TestOptunaSearcher.test_optuna_searcher_initialization ____________\n\nself = <test_search_components.TestOptunaSearcher object at 0x0000025ED5115D10>\n\n    def test_optuna_searcher_initialization(self):\n        \"\"\"Test OptunaSearcher initialization.\"\"\"\n        searcher = OptunaSearcher(self.config_space)\n    \n        assert searcher.config_spaces == self.config_space\n        assert searcher.study is not None\n>       assert searcher.trial_history == []\n               ^^^^^^^^^^^^^^^^^^^^^^\nE       AttributeError: 'OptunaSearcher' object has no attribute 'trial_history'\n\ntests\\test_search_components.py:47: AttributeError\n---------------------------- Captured stderr call -----------------------------\n[I 2025-08-02 18:29:28,307] A new study created in memory with name: no-name-48bc533a-d4a0-4253-b4e4-d4476624dbd9\n__________ TestOptunaSearcher.test_optuna_searcher_with_custom_study __________\n\nself = <test_search_components.TestOptunaSearcher object at 0x0000025ED5116FD0>\n\n    def test_optuna_searcher_with_custom_study(self):\n        \"\"\"Test OptunaSearcher with custom study.\"\"\"\n        import optuna\n    \n        study = optuna.create_study(direction=\"maximize\")\n>       searcher = OptunaSearcher(self.config_space, study=study)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nE       TypeError: OptunaSearcher.__init__() got an unexpected keyword argument 'study'\n\ntests\\test_search_components.py:54: TypeError\n---------------------------- Captured stderr call -----------------------------\n[I 2025-08-02 18:29:29,188] A new study created in memory with name: no-name-a6098ec0-ae42-4302-8769-ff956a6c97c2\n_________________ TestOptunaSearcher.test_suggest_parameters __________________\n\nself = <test_search_components.TestOptunaSearcher object at 0x0000025ED515DF30>\n\n    def test_suggest_parameters(self):\n        \"\"\"Test parameter suggestion functionality.\"\"\"\n        searcher = OptunaSearcher(self.config_space)\n    \n        # Test suggesting parameters for random_forest\n        params = searcher.suggest(\"trial_1\", \"random_forest\")\n    \n>       assert \"learner_name\" in params\nE       AssertionError: assert 'learner_name' in {'criterion': 'entropy', 'max_depth': 3, 'n_estimators': 64}\n\ntests\\test_search_components.py:65: AssertionError\n---------------------------- Captured stderr call -----------------------------\n[I 2025-08-02 18:29:29,206] A new study created in memory with name: no-name-9756a963-0bb5-47a8-9071-a6993b021d56\n________ TestOptunaSearcher.test_suggest_parameters_different_learner _________\n\nself = <test_search_components.TestOptunaSearcher object at 0x0000025ED515E190>\n\n    def test_suggest_parameters_different_learner(self):\n        \"\"\"Test parameter suggestion for different learners.\"\"\"\n        searcher = OptunaSearcher(self.config_space)\n    \n        # Test suggesting parameters for logistic_regression\n        params = searcher.suggest(\"trial_2\", \"logistic_regression\")\n    \n>       assert \"learner_name\" in params\nE       AssertionError: assert 'learner_name' in {'C': 2.1784687907056153, 'max_iter': 886}\n\ntests\\test_search_components.py:83: AssertionError\n---------------------------- Captured stderr call -----------------------------\n[I 2025-08-02 18:29:29,228] A new study created in memory with name: no-name-56adbcec-210f-4dc4-8b40-2642acd008f1\n___________________ TestOptunaSearcher.test_report_results ____________________\n\nself = <test_search_components.TestOptunaSearcher object at 0x0000025ED51E5130>\n\n    def test_report_results(self):\n        \"\"\"Test reporting trial results.\"\"\"\n        searcher = OptunaSearcher(self.config_space)\n    \n        # Suggest parameters\n        params = searcher.suggest(\"trial_1\", \"random_forest\")\n    \n        # Report results\n        searcher.report(\"trial_1\", 0.85)\n    \n        # Check that trial was recorded\n>       assert len(searcher.trial_history) == 1\n                   ^^^^^^^^^^^^^^^^^^^^^^\nE       AttributeError: 'OptunaSearcher' object has no attribute 'trial_history'\n\ntests\\test_search_components.py:103: AttributeError\n---------------------------- Captured stderr call -----------------------------\n[I 2025-08-02 18:29:29,248] A new study created in memory with name: no-name-bb97e96a-9f89-4d40-8eb3-e91195cf5194\n__________________ TestOptunaSearcher.test_get_best_results ___________________\n\nself = <test_search_components.TestOptunaSearcher object at 0x0000025ED51B5480>\n\n    def test_get_best_results(self):\n        \"\"\"Test getting best results.\"\"\"\n        searcher = OptunaSearcher(self.config_space)\n    \n        # Run multiple trials\n        for i in range(3):\n            params = searcher.suggest(f\"trial_{i}\", \"random_forest\")\n            score = 0.7 + i * 0.1  # Increasing scores\n            searcher.report(f\"trial_{i}\", score)\n    \n        # Get best results\n        best = searcher.get_best()\n    \n>       assert \"score\" in best\nE       AssertionError: assert 'score' in {'criterion': 'gini', 'max_depth': 6, 'n_estimators': 33}\n\ntests\\test_search_components.py:121: AssertionError\n---------------------------- Captured stderr call -----------------------------\n[I 2025-08-02 18:29:29,278] A new study created in memory with name: no-name-5056bb55-2f7b-40ce-a950-897c14199709\n________________ TestOptunaSearcher.test_invalid_learner_name _________________\n\nself = <test_search_components.TestOptunaSearcher object at 0x0000025ED51B56A0>\n\n    def test_invalid_learner_name(self):\n        \"\"\"Test handling of invalid learner names.\"\"\"\n        searcher = OptunaSearcher(self.config_space)\n    \n        with pytest.raises(ValueError, match=\"Unknown learner\"):\n>           searcher.suggest(\"trial_1\", \"invalid_learner\")\n\ntests\\test_search_components.py:131: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nself = <ml_teammate.search.optuna_search.OptunaSearcher object at 0x0000025ED5316AD0>\ntrial_id = 'trial_1', learner_name = 'invalid_learner'\n\n    def suggest(self, trial_id: str, learner_name: str) -> Dict:\n        trial = self.study.ask()\n        self.trials[trial_id] = trial\n    \n>       space = self.config_spaces[learner_name]\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nE       KeyError: 'invalid_learner'\n\nml_teammate\\search\\optuna_search.py:27: KeyError\n---------------------------- Captured stderr call -----------------------------\n[I 2025-08-02 18:29:29,297] A new study created in memory with name: no-name-92d43a9c-eb46-4256-96c3-a83cc8b64ecf\n_________________ TestOptunaSearcher.test_empty_config_space __________________\n\nself = <test_search_components.TestOptunaSearcher object at 0x0000025ED5122F50>\n\n    def test_empty_config_space(self):\n        \"\"\"Test handling of empty configuration space.\"\"\"\n>       with pytest.raises(ValueError, match=\"Empty configuration space\"):\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nE       Failed: DID NOT RAISE <class 'ValueError'>\n\ntests\\test_search_components.py:135: Failed\n---------------------------- Captured stderr call -----------------------------\n[I 2025-08-02 18:29:29,312] A new study created in memory with name: no-name-259480d8-75ab-49b1-9569-3bbecfb95b8f\n_________________ TestFLAMLSearcher.test_convert_config_space _________________\n\nself = <test_search_components.TestFLAMLSearcher object at 0x0000025ED515E2C0>\n\n    def test_convert_config_space(self):\n        \"\"\"Test configuration space conversion.\"\"\"\n        searcher = FLAMLSearcher(self.config_space)\n    \n        converted = searcher._convert_config_space(\"random_forest\")\n    \n        assert \"n_estimators\" in converted\n        assert \"max_depth\" in converted\n>       assert isinstance(converted[\"n_estimators\"], tuple)\nE       AssertionError: assert False\nE        +  where False = isinstance({'domain': 'uniform', 'high': 100, 'init_value': 55, 'low': 10}, tuple)\n\ntests\\test_search_components.py:183: AssertionError\n______________________ TestFLAMLSearcher.test_flaml_fit _______________________\n\nself = <test_search_components.TestFLAMLSearcher object at 0x0000025ED515E3F0>\n\n    def test_flaml_fit(self):\n        \"\"\"Test FLAML fitting functionality.\"\"\"\n        searcher = FLAMLSearcher(self.config_space, time_budget=5)\n    \n        # Generate test data\n        X, y = make_classification(n_samples=50, n_features=5, random_state=42)\n    \n        # Mock FLAML AutoML\n        with patch('ml_teammate.search.flaml_search.AutoML') as mock_automl:\n            mock_automl_instance = Mock()\n            mock_automl.return_value = mock_automl_instance\n    \n>           searcher.fit(X, y, task=\"classification\")\n\ntests\\test_search_components.py:198: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\nml_teammate\\search\\flaml_search.py:220: in fit\n    self.automl.fit(X, y, **self.settings)\n..\\..\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\flaml\\automl\\automl.py:1983: in fit\n    self._search()\n..\\..\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\flaml\\automl\\automl.py:2533: in _search\n    self._search_sequential()\n..\\..\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\flaml\\automl\\automl.py:2358: in _search_sequential\n    analysis = tune.run(\n..\\..\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\flaml\\tune\\tune.py:894: in run\n    result = evaluation_function(trial_to_run.config)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n..\\..\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\flaml\\automl\\state.py:306: in _compute_with_config_base\n    ) = compute_estimator(\n..\\..\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\flaml\\automl\\ml.py:382: in compute_estimator\n    val_loss, metric_for_logging, train_time, pred_time = task.evaluate_model_CV(\n..\\..\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\flaml\\automl\\task\\generic_task.py:785: in evaluate_model_CV\n    val_loss_i, metric_i, train_time_i, pred_time_i = get_val_loss(\n..\\..\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\flaml\\automl\\ml.py:516: in get_val_loss\n    estimator.fit(X_train, y_train, budget=budget, free_mem_ratio=free_mem_ratio, **fit_kwargs)\n..\\..\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\flaml\\automl\\model.py:1597: in fit\n    self._fit(X_train, y_train, callbacks=callbacks, **kwargs)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nself = <flaml.automl.model.LGBMEstimator object at 0x0000025ED5310AD0>\nX_train = array([[ 0.78536193,  1.58014031,  1.30714275, -0.48950622,  0.22517433],\n       [ 4.02272665,  2.47468454,  0.3464482...611, -0.78325329,  1.23051266, -0.5572771 ],\n       [-1.16921533,  1.14486264, -0.97468167,  1.58307723, -0.69470526]])\ny_train = array([0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0,\n       0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1])\nkwargs = {'callbacks': [functools.partial(<bound method LGBMEstimator._callback of <flaml.automl.model.LGBMEstimator object at 0x0000025ED5310AD0>>, 1754148569.35101, 1754148570.3493676, 0)], 'mode': 'max'}\ncurrent_time = 1754148569.3510187\nmodel = LGBMClassifier(learning_rate=np.float64(0.09999999999999995), max_bin=255,\n               n_estimators=4, n_jobs=-1, num_leaves=4,\n               reg_alpha=np.float64(0.0009765625), reg_lambda=np.float64(1.0),\n               verbose=-1)\n\n    def _fit(self, X_train, y_train, **kwargs):\n        current_time = time.time()\n        if \"groups\" in kwargs:\n            kwargs = kwargs.copy()\n            groups = kwargs.pop(\"groups\")\n            if self._task == \"rank\":\n                kwargs[\"group\"] = group_counts(groups)\n                # groups_val = kwargs.get('groups_val')\n                # if groups_val is not None:\n                #     kwargs['eval_group'] = [group_counts(groups_val)]\n                #     kwargs['eval_set'] = [\n                #         (kwargs['X_val'], kwargs['y_val'])]\n                #     kwargs['verbose'] = False\n                #     del kwargs['groups_val'], kwargs['X_val'], kwargs['y_val']\n        X_train = self._preprocess(X_train)\n        model = self.estimator_class(**self.params)\n        if logger.level == logging.DEBUG:\n            # xgboost 1.6 doesn't display all the params in the model str\n            logger.debug(f\"flaml.automl.model - {model} fit started with params {self.params}\")\n>       model.fit(X_train, y_train, **kwargs)\nE       TypeError: LGBMClassifier.fit() got an unexpected keyword argument 'mode'\n\n..\\..\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\flaml\\automl\\model.py:245: TypeError\n___________________ TestFLAMLSearcher.test_get_best_results ___________________\n\nself = <test_search_components.TestFLAMLSearcher object at 0x0000025ED51E6450>\n\n    def test_get_best_results(self):\n        \"\"\"Test getting best results from FLAML.\"\"\"\n        searcher = FLAMLSearcher(self.config_space)\n    \n        # Mock FLAML results\n        searcher.automl = Mock()\n        searcher.automl.best_config = {\"n_estimators\": 50, \"max_depth\": 5}\n        searcher.automl.best_loss = 0.1\n        searcher.automl.best_estimator = RandomForestClassifier()\n    \n        best = searcher.get_best()\n    \n>       assert \"score\" in best\nE       AssertionError: assert 'score' in {}\n\ntests\\test_search_components.py:215: AssertionError\n____________ TestEarlyConvergenceIndicator.test_eci_initialization ____________\n\nself = <test_search_components.TestEarlyConvergenceIndicator object at 0x0000025ED5117250>\n\n    def test_eci_initialization(self):\n        \"\"\"Test ECI initialization.\"\"\"\n        assert self.eci.window_size == 5\n        assert self.eci.min_trials == 3\n        assert self.eci.improvement_threshold == 0.001\n        assert self.eci.patience == 3\n>       assert self.eci.trials == []\n               ^^^^^^^^^^^^^^^\nE       AttributeError: 'EarlyConvergenceIndicator' object has no attribute 'trials'\n\ntests\\test_search_components.py:238: AttributeError\n________________ TestEarlyConvergenceIndicator.test_add_trial _________________\n\nself = <test_search_components.TestEarlyConvergenceIndicator object at 0x0000025ED5117390>\n\n    def test_add_trial(self):\n        \"\"\"Test adding trials to ECI.\"\"\"\n        self.eci.add_trial(\"trial_1\", 0.8)\n        self.eci.add_trial(\"trial_2\", 0.85)\n    \n>       assert len(self.eci.trials) == 2\n                   ^^^^^^^^^^^^^^^\nE       AttributeError: 'EarlyConvergenceIndicator' object has no attribute 'trials'\n\ntests\\test_search_components.py:245: AttributeError\n_______ TestEarlyConvergenceIndicator.test_improvement_rate_convergence _______\n\nself = <test_search_components.TestEarlyConvergenceIndicator object at 0x0000025ED515E520>\n\n    def test_improvement_rate_convergence(self):\n        \"\"\"Test improvement rate convergence detection.\"\"\"\n        eci = EarlyConvergenceIndicator(convergence_method=\"improvement_rate\")\n    \n        # Add trials with decreasing improvement\n        scores = [0.8, 0.82, 0.83, 0.835, 0.836, 0.8365, 0.8365, 0.8365]\n        for i, score in enumerate(scores):\n            eci.add_trial(f\"trial_{i}\", score)\n    \n        # Should converge (very small improvements)\n>       assert eci.should_stop()\nE       assert False\nE        +  where False = should_stop()\nE        +    where should_stop = <ml_teammate.search.eci.EarlyConvergenceIndicator object at 0x0000025ED5316FD0>.should_stop\n\ntests\\test_search_components.py:277: AssertionError\n___________ TestEarlyConvergenceIndicator.test_get_convergence_info ___________\n\nself = <test_search_components.TestEarlyConvergenceIndicator object at 0x0000025ED51B57B0>\n\n    def test_get_convergence_info(self):\n        \"\"\"Test getting convergence information.\"\"\"\n        # Add some trials\n        for i in range(5):\n            self.eci.add_trial(f\"trial_{i}\", 0.8 + i * 0.01)\n    \n        info = self.eci.get_convergence_info()\n    \n        assert \"n_trials\" in info\n        assert \"converged\" in info\n>       assert \"method\" in info\nE       AssertionError: assert 'method' in {'best_score': 0.8400000000000001, 'best_trial': 'trial_4', 'consecutive_no_improvement': 0, 'converged': False, ...}\n\ntests\\test_search_components.py:312: AssertionError\n__________________ TestEarlyConvergenceIndicator.test_reset ___________________\n\nself = <test_search_components.TestEarlyConvergenceIndicator object at 0x0000025ED5123050>\n\n    def test_reset(self):\n        \"\"\"Test resetting ECI state.\"\"\"\n        # Add some trials\n        for i in range(3):\n            self.eci.add_trial(f\"trial_{i}\", 0.8 + i * 0.01)\n    \n>       assert len(self.eci.trials) == 3\n                   ^^^^^^^^^^^^^^^\nE       AttributeError: 'EarlyConvergenceIndicator' object has no attribute 'trials'\n\ntests\\test_search_components.py:321: AttributeError\n________ TestMultiObjectiveECI.test_multi_objective_eci_initialization ________\n\nself = <test_search_components.TestMultiObjectiveECI object at 0x0000025ED5117610>\n\n    def test_multi_objective_eci_initialization(self):\n        \"\"\"Test MultiObjectiveECI initialization.\"\"\"\n        eci = MultiObjectiveECI(objectives=[\"accuracy\", \"speed\"])\n    \n        assert eci.objectives == [\"accuracy\", \"speed\"]\n>       assert len(eci.trials) == 0\n                   ^^^^^^^^^^\nE       AttributeError: 'MultiObjectiveECI' object has no attribute 'trials'\n\ntests\\test_search_components.py:361: AttributeError\n__________ TestMultiObjectiveECI.test_multi_objective_trial_addition __________\n\nself = <test_search_components.TestMultiObjectiveECI object at 0x0000025ED5117750>\n\n    def test_multi_objective_trial_addition(self):\n        \"\"\"Test adding multi-objective trials.\"\"\"\n        eci = MultiObjectiveECI(objectives=[\"accuracy\", \"speed\"])\n    \n        # Add trial with multiple objectives\n        eci.add_trial(\"trial_1\", {\"accuracy\": 0.85, \"speed\": 0.9})\n    \n>       assert len(eci.trials) == 1\n                   ^^^^^^^^^^\nE       AttributeError: 'MultiObjectiveECI' object has no attribute 'trials'\n\ntests\\test_search_components.py:370: AttributeError\n__________ TestSearchFactoryFunctions.test_list_available_searchers ___________\n\nself = <test_search_components.TestSearchFactoryFunctions object at 0x0000025ED515DCD0>\n\n    def test_list_available_searchers(self):\n        \"\"\"Test list_available_searchers function.\"\"\"\n        searchers = list_available_searchers()\n    \n        assert \"optuna\" in searchers\n        assert \"flaml\" in searchers\n>       assert \"flaml_time_budget\" in searchers\nE       AssertionError: assert 'flaml_time_budget' in {'flaml': {'dependencies': ['flaml'], 'description': 'FLAML-based hyperparameter optimization', 'features': ['Time budget', 'Resource management', 'Early stopping']}, 'flaml_resource': {'dependencies': ['flaml'], 'description': 'FLAML with resource awareness', 'features': ['Memory budget', 'Computational constraints']}, 'flaml_time': {'dependencies': ['flaml'], 'description': 'FLAML with time budget focus', 'features': ['Time-bounded optimization', 'Fast convergence']}, 'optuna': {'dependencies': ['optuna'], 'description': 'Optuna-based hyperparameter optimization', 'features': ['TPE sampler', 'Random sampler', 'Multi-objective', 'Pruning']}}\n\ntests\\test_search_components.py:417: AssertionError\n============================== warnings summary ===============================\n..\\..\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pydantic\\_internal\\_config.py:323\n  C:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pydantic\\_internal\\_config.py:323: PydanticDeprecatedSince20: Support for class-based `config` is deprecated, use ConfigDict instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/\n    warnings.warn(DEPRECATION_MESSAGE, DeprecationWarning)\n\n-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html\n=========================== short test summary info ===========================\nFAILED tests/test_search_components.py::TestOptunaSearcher::test_optuna_searcher_initialization\nFAILED tests/test_search_components.py::TestOptunaSearcher::test_optuna_searcher_with_custom_study\nFAILED tests/test_search_components.py::TestOptunaSearcher::test_suggest_parameters\nFAILED tests/test_search_components.py::TestOptunaSearcher::test_suggest_parameters_different_learner\nFAILED tests/test_search_components.py::TestOptunaSearcher::test_report_results\nFAILED tests/test_search_components.py::TestOptunaSearcher::test_get_best_results\nFAILED tests/test_search_components.py::TestOptunaSearcher::test_invalid_learner_name\nFAILED tests/test_search_components.py::TestOptunaSearcher::test_empty_config_space\nFAILED tests/test_search_components.py::TestFLAMLSearcher::test_convert_config_space\nFAILED tests/test_search_components.py::TestFLAMLSearcher::test_flaml_fit - T...\nFAILED tests/test_search_components.py::TestFLAMLSearcher::test_get_best_results\nFAILED tests/test_search_components.py::TestEarlyConvergenceIndicator::test_eci_initialization\nFAILED tests/test_search_components.py::TestEarlyConvergenceIndicator::test_add_trial\nFAILED tests/test_search_components.py::TestEarlyConvergenceIndicator::test_improvement_rate_convergence\nFAILED tests/test_search_components.py::TestEarlyConvergenceIndicator::test_get_convergence_info\nFAILED tests/test_search_components.py::TestEarlyConvergenceIndicator::test_reset\nFAILED tests/test_search_components.py::TestMultiObjectiveECI::test_multi_objective_eci_initialization\nFAILED tests/test_search_components.py::TestMultiObjectiveECI::test_multi_objective_trial_addition\nFAILED tests/test_search_components.py::TestSearchFactoryFunctions::test_list_available_searchers\n================== 19 failed, 10 passed, 1 warning in 13.95s ==================\n",
        "stderr": ""
      },
      "test_learner_registry.py": {
        "success": false,
        "returncode": 1,
        "stdout": "============================= test session starts =============================\nplatform win32 -- Python 3.13.2, pytest-8.4.1, pluggy-1.6.0 -- C:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python313\\python.exe\ncachedir: .pytest_cache\nrootdir: C:\\Users\\ADMIN\\Desktop\\MLTeammate\nconfigfile: pytest.ini\nplugins: anyio-4.9.0, cov-6.2.1\ncollecting ... collected 37 items\n\ntests/test_learner_registry.py::TestSklearnWrapper::test_sklearn_wrapper_initialization FAILED [  2%]\ntests/test_learner_registry.py::TestSklearnWrapper::test_sklearn_wrapper_initialization_no_config PASSED [  5%]\ntests/test_learner_registry.py::TestSklearnWrapper::test_create_model FAILED [  8%]\ntests/test_learner_registry.py::TestSklearnWrapper::test_fit PASSED      [ 10%]\ntests/test_learner_registry.py::TestSklearnWrapper::test_predict PASSED  [ 13%]\ntests/test_learner_registry.py::TestSklearnWrapper::test_predict_proba PASSED [ 16%]\ntests/test_learner_registry.py::TestSklearnWrapper::test_predict_proba_not_supported PASSED [ 18%]\ntests/test_learner_registry.py::TestSklearnWrapper::test_get_params PASSED [ 21%]\ntests/test_learner_registry.py::TestSklearnWrapper::test_set_params PASSED [ 24%]\ntests/test_learner_registry.py::TestSklearnWrapper::test_predict_without_fit PASSED [ 27%]\ntests/test_learner_registry.py::TestSklearnWrapper::test_predict_proba_without_fit PASSED [ 29%]\ntests/test_learner_registry.py::TestLearnerRegistry::test_registry_initialization PASSED [ 32%]\ntests/test_learner_registry.py::TestLearnerRegistry::test_register_learner PASSED [ 35%]\ntests/test_learner_registry.py::TestLearnerRegistry::test_get_learner PASSED [ 37%]\ntests/test_learner_registry.py::TestLearnerRegistry::test_get_config_space PASSED [ 40%]\ntests/test_learner_registry.py::TestLearnerRegistry::test_get_all_learners PASSED [ 43%]\ntests/test_learner_registry.py::TestLearnerRegistry::test_get_classification_learners PASSED [ 45%]\ntests/test_learner_registry.py::TestLearnerRegistry::test_get_regression_learners PASSED [ 48%]\ntests/test_learner_registry.py::TestLearnerRegistry::test_check_dependencies PASSED [ 51%]\ntests/test_learner_registry.py::TestLearnerRegistry::test_create_learners_dict PASSED [ 54%]\ntests/test_learner_registry.py::TestLearnerRegistry::test_create_config_space PASSED [ 56%]\ntests/test_learner_registry.py::TestLearnerRegistry::test_create_learners_dict_invalid_learner PASSED [ 59%]\ntests/test_learner_registry.py::TestLearnerRegistry::test_create_config_space_invalid_learner PASSED [ 62%]\ntests/test_learner_registry.py::TestLearnerRegistry::test_pre_built_custom_learners PASSED [ 64%]\ntests/test_learner_registry.py::TestLearnerRegistry::test_create_ensemble_learner PASSED [ 67%]\ntests/test_learner_registry.py::TestRegistryFactoryFunctions::test_get_learner_registry PASSED [ 70%]\ntests/test_learner_registry.py::TestRegistryFactoryFunctions::test_get_learner PASSED [ 72%]\ntests/test_learner_registry.py::TestRegistryFactoryFunctions::test_get_config_space PASSED [ 75%]\ntests/test_learner_registry.py::TestRegistryFactoryFunctions::test_get_all_learners PASSED [ 78%]\ntests/test_learner_registry.py::TestRegistryFactoryFunctions::test_get_classification_learners PASSED [ 81%]\ntests/test_learner_registry.py::TestRegistryFactoryFunctions::test_get_regression_learners PASSED [ 83%]\ntests/test_learner_registry.py::TestRegistryFactoryFunctions::test_create_learners_dict PASSED [ 86%]\ntests/test_learner_registry.py::TestRegistryFactoryFunctions::test_create_config_space PASSED [ 89%]\ntests/test_learner_registry.py::TestLearnerIntegration::test_random_forest_integration FAILED [ 91%]\ntests/test_learner_registry.py::TestLearnerIntegration::test_logistic_regression_integration FAILED [ 94%]\ntests/test_learner_registry.py::TestLearnerIntegration::test_custom_learners_integration FAILED [ 97%]\ntests/test_learner_registry.py::TestLearnerIntegration::test_ensemble_integration PASSED [100%]\n\n================================== FAILURES ===================================\n___________ TestSklearnWrapper.test_sklearn_wrapper_initialization ____________\n\nself = <test_learner_registry.TestSklearnWrapper object at 0x000001D48F0C2FD0>\n\n    def test_sklearn_wrapper_initialization(self):\n        \"\"\"Test SklearnWrapper initialization.\"\"\"\n        wrapper = SklearnWrapper(RandomForestClassifier, {\"n_estimators\": 100})\n    \n        assert wrapper.model_class == RandomForestClassifier\n        assert wrapper.config == {\"n_estimators\": 100}\n>       assert wrapper.model is None\nE       assert RandomForestClassifier() is None\nE        +  where RandomForestClassifier() = SklearnWrapper(n_estimators=100).model\n\ntests\\test_learner_registry.py:48: AssertionError\n____________________ TestSklearnWrapper.test_create_model _____________________\n\nself = <test_learner_registry.TestSklearnWrapper object at 0x000001D4B1B0B230>\n\n    def test_create_model(self):\n        \"\"\"Test model creation.\"\"\"\n        wrapper = SklearnWrapper(RandomForestClassifier, {\"n_estimators\": 50, \"random_state\": 42})\n    \n        model = wrapper._create_model()\n    \n>       assert isinstance(model, RandomForestClassifier)\nE       assert False\nE        +  where False = isinstance(None, RandomForestClassifier)\n\ntests\\test_learner_registry.py:64: AssertionError\n____________ TestLearnerIntegration.test_random_forest_integration ____________\n\nself = <test_learner_registry.TestLearnerIntegration object at 0x000001D4B1C69310>\n\n    def test_random_forest_integration(self):\n        \"\"\"Test Random Forest learner integration.\"\"\"\n        factory = get_learner(\"random_forest\")\n        config_space = get_config_space(\"random_forest\")\n    \n        # Test factory function\n        model = factory({\"n_estimators\": 10, \"random_state\": 42})\n>       assert isinstance(model, RandomForestClassifier)\nE       assert False\nE        +  where False = isinstance(SklearnWrapper(n_estimators=10, random_state=42), RandomForestClassifier)\n\ntests\\test_learner_registry.py:396: AssertionError\n_________ TestLearnerIntegration.test_logistic_regression_integration _________\n\nself = <test_learner_registry.TestLearnerIntegration object at 0x000001D4B1C69450>\n\n    def test_logistic_regression_integration(self):\n        \"\"\"Test Logistic Regression learner integration.\"\"\"\n        factory = get_learner(\"logistic_regression\")\n        config_space = get_config_space(\"logistic_regression\")\n    \n        # Test factory function\n        model = factory({\"C\": 1.0, \"random_state\": 42})\n>       assert isinstance(model, LogisticRegression)\nE       assert False\nE        +  where False = isinstance(SklearnWrapper(C=1.0, random_state=42), LogisticRegression)\n\ntests\\test_learner_registry.py:414: AssertionError\n___________ TestLearnerIntegration.test_custom_learners_integration ___________\n\nself = <test_learner_registry.TestLearnerIntegration object at 0x000001D4B1CAC2B0>\n\n    def test_custom_learners_integration(self):\n        \"\"\"Test custom learners integration.\"\"\"\n        # Test custom_rf\n        factory = get_learner(\"custom_rf\")\n        config_space = get_config_space(\"custom_rf\")\n    \n        model = factory({\"n_estimators\": 50, \"max_depth\": 5})\n>       assert isinstance(model, RandomForestClassifier)\nE       assert False\nE        +  where False = isinstance(SklearnWrapper(max_depth=5, min_samples_split=2, n_estimators=50,\\n               random_state=42), RandomForestClassifier)\n\ntests\\test_learner_registry.py:432: AssertionError\n=========================== short test summary info ===========================\nFAILED tests/test_learner_registry.py::TestSklearnWrapper::test_sklearn_wrapper_initialization\nFAILED tests/test_learner_registry.py::TestSklearnWrapper::test_create_model\nFAILED tests/test_learner_registry.py::TestLearnerIntegration::test_random_forest_integration\nFAILED tests/test_learner_registry.py::TestLearnerIntegration::test_logistic_regression_integration\nFAILED tests/test_learner_registry.py::TestLearnerIntegration::test_custom_learners_integration\n======================== 5 failed, 32 passed in 5.73s =========================\n",
        "stderr": ""
      },
      "test_simple_api.py": {
        "success": false,
        "returncode": 1,
        "stdout": "============================= test session starts =============================\nplatform win32 -- Python 3.13.2, pytest-8.4.1, pluggy-1.6.0 -- C:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python313\\python.exe\ncachedir: .pytest_cache\nrootdir: C:\\Users\\ADMIN\\Desktop\\MLTeammate\nconfigfile: pytest.ini\nplugins: anyio-4.9.0, cov-6.2.1\ncollecting ... collected 10 items\n\ntests/test_simple_api.py::TestSimpleAPI::test_list_available_learners PASSED [ 10%]\ntests/test_simple_api.py::TestSimpleAPI::test_get_learner_info FAILED    [ 20%]\ntests/test_simple_api.py::TestSimpleAPI::test_simple_automl_classification FAILED [ 30%]\ntests/test_simple_api.py::TestSimpleAPI::test_simple_automl_regression FAILED [ 40%]\ntests/test_simple_api.py::TestSimpleAPI::test_single_learner FAILED      [ 50%]\ntests/test_simple_api.py::TestSimpleAPI::test_quick_classification FAILED [ 60%]\ntests/test_simple_api.py::TestSimpleAPI::test_quick_regression FAILED    [ 70%]\ntests/test_simple_api.py::TestSimpleAPI::test_results_summary FAILED     [ 80%]\ntests/test_simple_api.py::TestSimpleAPI::test_invalid_learner FAILED     [ 90%]\ntests/test_simple_api.py::TestSimpleAPI::test_default_learners FAILED    [100%]\n\n================================== FAILURES ===================================\n_____________________ TestSimpleAPI.test_get_learner_info _____________________\n\nself = <test_simple_api.TestSimpleAPI object at 0x000002751C16E350>\n\n    def test_get_learner_info(self):\n        \"\"\"Test getting information about learners.\"\"\"\n        # Test classification learner\n        rf_info = get_learner_info(\"random_forest\")\n        assert \"error\" not in rf_info\n        assert rf_info[\"name\"] == \"random_forest\"\n>       assert rf_info[\"is_classification\"] is True\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nE       KeyError: 'is_classification'\n\ntests\\test_simple_api.py:56: KeyError\n_______________ TestSimpleAPI.test_simple_automl_classification _______________\n\nself = <test_simple_api.TestSimpleAPI object at 0x000002751C13A8B0>\n\n    def test_simple_automl_classification(self):\n        \"\"\"Test SimpleAutoML with classification task.\"\"\"\n        # Generate test data\n        X, y = make_classification(n_samples=100, n_features=10, random_state=42)\n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n    \n        # Create SimpleAutoML instance\n        automl = SimpleAutoML(\n            learners=[\"random_forest\", \"logistic_regression\"],\n            task=\"classification\",\n            n_trials=2,\n            cv=2\n        )\n    \n        # Fit the model\n>       automl.fit(X_train, y_train)\n\ntests\\test_simple_api.py:86: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\nml_teammate\\interface\\simple_api.py:445: in fit\n    self.controller.fit(X, y)\nml_teammate\\automl\\controller.py:86: in fit\n    cb.on_experiment_end(self.best_score, self.searcher.get_best())\n                                          ^^^^^^^^^^^^^^^^^^^^^^^^\nml_teammate\\search\\optuna_search.py:56: in get_best\n    return self.study.best_trial.params\n           ^^^^^^^^^^^^^^^^^^^^^\n..\\..\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\optuna\\study\\study.py:155: in best_trial\n    return self._get_best_trial(deepcopy=True)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n..\\..\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\optuna\\study\\study.py:307: in _get_best_trial\n    best_trial = self._storage.get_best_trial(self._study_id)\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nself = <optuna.storages._in_memory.InMemoryStorage object at 0x000002751C22D550>\nstudy_id = 0\n\n    def get_best_trial(self, study_id: int) -> FrozenTrial:\n        with self._lock:\n            self._check_study_id(study_id)\n    \n            best_trial_id = self._studies[study_id].best_trial_id\n    \n            if best_trial_id is None:\n>               raise ValueError(\"No trials are completed yet.\")\nE               ValueError: No trials are completed yet.\n\n..\\..\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\optuna\\storages\\_in_memory.py:252: ValueError\n---------------------------- Captured stdout call -----------------------------\n\\U0001f3af Progress tracking enabled for 2 trials\\n[2025-08-02 18:29:57] [INFO] \\U0001f680 Starting MLTeammate experiment: None\\n[2025-08-02 18:29:57] [INFO] \\U0001f4cb Experiment config: {\\n  \"task\": \"classification\",\\n  \"n_trials\": 2,\\n  \"cv\": 2,\\n  \"learners\": [\\n    \"random_forest\",\\n    \"logistic_regression\"\\n  ],\\n  \"data_shape\": [\\n    80,\\n    10\\n  ]\\n}\\n[Warning] Trial 1 failed: 'xgboost'\\n[Warning] Trial 2 failed: 'xgboost'\n---------------------------- Captured stderr call -----------------------------\n[I 2025-08-02 18:29:57,774] A new study created in memory with name: no-name-04403ade-5220-4ed3-996a-346d0c267719\n_________________ TestSimpleAPI.test_simple_automl_regression _________________\n\nself = <test_simple_api.TestSimpleAPI object at 0x000002751C13AB10>\n\n    def test_simple_automl_regression(self):\n        \"\"\"Test SimpleAutoML with regression task.\"\"\"\n        # Generate test data\n        X, y = make_regression(n_samples=100, n_features=10, random_state=42)\n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n    \n        # Create SimpleAutoML instance\n        automl = SimpleAutoML(\n            learners=[\"random_forest_regressor\", \"linear_regression\"],\n            task=\"regression\",\n            n_trials=2,\n            cv=2\n        )\n    \n        # Fit the model\n>       automl.fit(X_train, y_train)\n\ntests\\test_simple_api.py:117: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\nml_teammate\\interface\\simple_api.py:445: in fit\n    self.controller.fit(X, y)\nml_teammate\\automl\\controller.py:86: in fit\n    cb.on_experiment_end(self.best_score, self.searcher.get_best())\n                                          ^^^^^^^^^^^^^^^^^^^^^^^^\nml_teammate\\search\\optuna_search.py:56: in get_best\n    return self.study.best_trial.params\n           ^^^^^^^^^^^^^^^^^^^^^\n..\\..\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\optuna\\study\\study.py:155: in best_trial\n    return self._get_best_trial(deepcopy=True)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n..\\..\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\optuna\\study\\study.py:307: in _get_best_trial\n    best_trial = self._storage.get_best_trial(self._study_id)\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nself = <optuna.storages._in_memory.InMemoryStorage object at 0x000002751C16FD90>\nstudy_id = 0\n\n    def get_best_trial(self, study_id: int) -> FrozenTrial:\n        with self._lock:\n            self._check_study_id(study_id)\n    \n            best_trial_id = self._studies[study_id].best_trial_id\n    \n            if best_trial_id is None:\n>               raise ValueError(\"No trials are completed yet.\")\nE               ValueError: No trials are completed yet.\n\n..\\..\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\optuna\\storages\\_in_memory.py:252: ValueError\n---------------------------- Captured stdout call -----------------------------\n\\U0001f3af Progress tracking enabled for 2 trials\\n[2025-08-02 18:29:57] [INFO] \\U0001f680 Starting MLTeammate experiment: None\\n[2025-08-02 18:29:57] [INFO] \\U0001f4cb Experiment config: {\\n  \"task\": \"regression\",\\n  \"n_trials\": 2,\\n  \"cv\": 2,\\n  \"learners\": [\\n    \"random_forest_regressor\",\\n    \"linear_regression\"\\n  ],\\n  \"data_shape\": [\\n    80,\\n    10\\n  ]\\n}\\n[Warning] Trial 1 failed: 'xgboost'\\n[Warning] Trial 2 failed: 'xgboost'\n---------------------------- Captured stderr call -----------------------------\n[I 2025-08-02 18:29:57,839] A new study created in memory with name: no-name-7f850bde-68c8-4cf3-b562-4f60a49c072c\n______________________ TestSimpleAPI.test_single_learner ______________________\n\nself = <test_simple_api.TestSimpleAPI object at 0x000002751C20CB90>\n\n    def test_single_learner(self):\n        \"\"\"Test SimpleAutoML with a single learner.\"\"\"\n        X, y = make_classification(n_samples=50, n_features=5, random_state=42)\n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n    \n        # Use single learner as string\n        automl = SimpleAutoML(\n            learners=\"random_forest\",\n            task=\"classification\",\n            n_trials=2,\n            cv=2\n        )\n    \n>       automl.fit(X_train, y_train)\n\ntests\\test_simple_api.py:141: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\nml_teammate\\interface\\simple_api.py:445: in fit\n    self.controller.fit(X, y)\nml_teammate\\automl\\controller.py:86: in fit\n    cb.on_experiment_end(self.best_score, self.searcher.get_best())\n                                          ^^^^^^^^^^^^^^^^^^^^^^^^\nml_teammate\\search\\optuna_search.py:56: in get_best\n    return self.study.best_trial.params\n           ^^^^^^^^^^^^^^^^^^^^^\n..\\..\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\optuna\\study\\study.py:155: in best_trial\n    return self._get_best_trial(deepcopy=True)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n..\\..\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\optuna\\study\\study.py:307: in _get_best_trial\n    best_trial = self._storage.get_best_trial(self._study_id)\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nself = <optuna.storages._in_memory.InMemoryStorage object at 0x000002751E46CF50>\nstudy_id = 0\n\n    def get_best_trial(self, study_id: int) -> FrozenTrial:\n        with self._lock:\n            self._check_study_id(study_id)\n    \n            best_trial_id = self._studies[study_id].best_trial_id\n    \n            if best_trial_id is None:\n>               raise ValueError(\"No trials are completed yet.\")\nE               ValueError: No trials are completed yet.\n\n..\\..\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\optuna\\storages\\_in_memory.py:252: ValueError\n---------------------------- Captured stdout call -----------------------------\n\\U0001f3af Progress tracking enabled for 2 trials\\n[2025-08-02 18:29:57] [INFO] \\U0001f680 Starting MLTeammate experiment: None\\n[2025-08-02 18:29:57] [INFO] \\U0001f4cb Experiment config: {\\n  \"task\": \"classification\",\\n  \"n_trials\": 2,\\n  \"cv\": 2,\\n  \"learners\": [\\n    \"random_forest\"\\n  ],\\n  \"data_shape\": [\\n    40,\\n    5\\n  ]\\n}\\n[Warning] Trial 1 failed: 'xgboost'\\n[Warning] Trial 2 failed: 'xgboost'\n---------------------------- Captured stderr call -----------------------------\n[I 2025-08-02 18:29:57,917] A new study created in memory with name: no-name-4a6f814d-6d5b-4e86-9442-d9a93d8a96b0\n___________________ TestSimpleAPI.test_quick_classification ___________________\n\nself = <test_simple_api.TestSimpleAPI object at 0x000002751C1A9BF0>\n\n    def test_quick_classification(self):\n        \"\"\"Test the quick_classification function.\"\"\"\n        X, y = make_classification(n_samples=50, n_features=5, random_state=42)\n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n    \n        # Use quick classification\n>       automl = quick_classification(\n            X_train, y_train,\n            learners=[\"random_forest\"],\n            n_trials=2,\n            cv=2\n        )\n\ntests\\test_simple_api.py:151: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\nml_teammate\\interface\\simple_api.py:528: in quick_classification\n    return automl.quick_classify(X, y)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\nml_teammate\\interface\\simple_api.py:294: in quick_classify\n    self.fit(X, y)\nml_teammate\\interface\\simple_api.py:445: in fit\n    self.controller.fit(X, y)\nml_teammate\\automl\\controller.py:86: in fit\n    cb.on_experiment_end(self.best_score, self.searcher.get_best())\n                                          ^^^^^^^^^^^^^^^^^^^^^^^^\nml_teammate\\search\\optuna_search.py:56: in get_best\n    return self.study.best_trial.params\n           ^^^^^^^^^^^^^^^^^^^^^\n..\\..\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\optuna\\study\\study.py:155: in best_trial\n    return self._get_best_trial(deepcopy=True)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n..\\..\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\optuna\\study\\study.py:307: in _get_best_trial\n    best_trial = self._storage.get_best_trial(self._study_id)\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nself = <optuna.storages._in_memory.InMemoryStorage object at 0x000002751E4EC640>\nstudy_id = 0\n\n    def get_best_trial(self, study_id: int) -> FrozenTrial:\n        with self._lock:\n            self._check_study_id(study_id)\n    \n            best_trial_id = self._studies[study_id].best_trial_id\n    \n            if best_trial_id is None:\n>               raise ValueError(\"No trials are completed yet.\")\nE               ValueError: No trials are completed yet.\n\n..\\..\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\optuna\\storages\\_in_memory.py:252: ValueError\n---------------------------- Captured stdout call -----------------------------\n\\U0001f680 Running Quick Classification...\\n========================================\\n\\U0001f3af Progress tracking enabled for 2 trials\\n[2025-08-02 18:29:58] [INFO] \\U0001f680 Starting MLTeammate experiment: None\\n[2025-08-02 18:29:58] [INFO] \\U0001f4cb Experiment config: {\\n  \"task\": \"classification\",\\n  \"n_trials\": 2,\\n  \"cv\": 2,\\n  \"learners\": [\\n    \"random_forest\"\\n  ],\\n  \"data_shape\": [\\n    40,\\n    5\\n  ]\\n}\\n[Warning] Trial 1 failed: 'xgboost'\\n[Warning] Trial 2 failed: 'xgboost'\n---------------------------- Captured stderr call -----------------------------\n[I 2025-08-02 18:29:58,160] A new study created in memory with name: no-name-23e524ae-0772-4589-baf1-81e3f71366e1\n_____________________ TestSimpleAPI.test_quick_regression _____________________\n\nself = <test_simple_api.TestSimpleAPI object at 0x000002751C1A9E10>\n\n    def test_quick_regression(self):\n        \"\"\"Test the quick_regression function.\"\"\"\n        X, y = make_regression(n_samples=50, n_features=5, random_state=42)\n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n    \n        # Use quick regression\n>       automl = quick_regression(\n            X_train, y_train,\n            learners=[\"random_forest_regressor\"],\n            n_trials=2,\n            cv=2\n        )\n\ntests\\test_simple_api.py:171: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\nml_teammate\\interface\\simple_api.py:557: in quick_regression\n    return automl.quick_regress(X, y)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\nml_teammate\\interface\\simple_api.py:314: in quick_regress\n    self.fit(X, y)\nml_teammate\\interface\\simple_api.py:445: in fit\n    self.controller.fit(X, y)\nml_teammate\\automl\\controller.py:86: in fit\n    cb.on_experiment_end(self.best_score, self.searcher.get_best())\n                                          ^^^^^^^^^^^^^^^^^^^^^^^^\nml_teammate\\search\\optuna_search.py:56: in get_best\n    return self.study.best_trial.params\n           ^^^^^^^^^^^^^^^^^^^^^\n..\\..\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\optuna\\study\\study.py:155: in best_trial\n    return self._get_best_trial(deepcopy=True)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n..\\..\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\optuna\\study\\study.py:307: in _get_best_trial\n    best_trial = self._storage.get_best_trial(self._study_id)\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nself = <optuna.storages._in_memory.InMemoryStorage object at 0x000002751E4EDA70>\nstudy_id = 0\n\n    def get_best_trial(self, study_id: int) -> FrozenTrial:\n        with self._lock:\n            self._check_study_id(study_id)\n    \n            best_trial_id = self._studies[study_id].best_trial_id\n    \n            if best_trial_id is None:\n>               raise ValueError(\"No trials are completed yet.\")\nE               ValueError: No trials are completed yet.\n\n..\\..\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\optuna\\storages\\_in_memory.py:252: ValueError\n---------------------------- Captured stdout call -----------------------------\n\\U0001f680 Running Quick Regression...\\n========================================\\n\\U0001f3af Progress tracking enabled for 2 trials\\n[2025-08-02 18:29:58] [INFO] \\U0001f680 Starting MLTeammate experiment: None\\n[2025-08-02 18:29:58] [INFO] \\U0001f4cb Experiment config: {\\n  \"task\": \"regression\",\\n  \"n_trials\": 2,\\n  \"cv\": 2,\\n  \"learners\": [\\n    \"random_forest_regressor\"\\n  ],\\n  \"data_shape\": [\\n    40,\\n    5\\n  ]\\n}\\n[Warning] Trial 1 failed: 'xgboost'\\n[Warning] Trial 2 failed: 'xgboost'\n---------------------------- Captured stderr call -----------------------------\n[I 2025-08-02 18:29:58,269] A new study created in memory with name: no-name-ffcb2c05-52f5-4b9b-a7ad-1a175fe7c80a\n_____________________ TestSimpleAPI.test_results_summary ______________________\n\nself = <test_simple_api.TestSimpleAPI object at 0x000002751C123450>\n\n    def test_results_summary(self):\n        \"\"\"Test getting results summary.\"\"\"\n        X, y = make_classification(n_samples=50, n_features=5, random_state=42)\n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n    \n        automl = SimpleAutoML(\n            learners=[\"random_forest\"],\n            task=\"classification\",\n            n_trials=2,\n            cv=2\n        )\n    \n>       automl.fit(X_train, y_train)\n\ntests\\test_simple_api.py:197: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\nml_teammate\\interface\\simple_api.py:445: in fit\n    self.controller.fit(X, y)\nml_teammate\\automl\\controller.py:86: in fit\n    cb.on_experiment_end(self.best_score, self.searcher.get_best())\n                                          ^^^^^^^^^^^^^^^^^^^^^^^^\nml_teammate\\search\\optuna_search.py:56: in get_best\n    return self.study.best_trial.params\n           ^^^^^^^^^^^^^^^^^^^^^\n..\\..\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\optuna\\study\\study.py:155: in best_trial\n    return self._get_best_trial(deepcopy=True)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n..\\..\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\optuna\\study\\study.py:307: in _get_best_trial\n    best_trial = self._storage.get_best_trial(self._study_id)\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nself = <optuna.storages._in_memory.InMemoryStorage object at 0x0000027575CDB410>\nstudy_id = 0\n\n    def get_best_trial(self, study_id: int) -> FrozenTrial:\n        with self._lock:\n            self._check_study_id(study_id)\n    \n            best_trial_id = self._studies[study_id].best_trial_id\n    \n            if best_trial_id is None:\n>               raise ValueError(\"No trials are completed yet.\")\nE               ValueError: No trials are completed yet.\n\n..\\..\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\optuna\\storages\\_in_memory.py:252: ValueError\n---------------------------- Captured stdout call -----------------------------\n\\U0001f3af Progress tracking enabled for 2 trials\\n[2025-08-02 18:29:58] [INFO] \\U0001f680 Starting MLTeammate experiment: None\\n[2025-08-02 18:29:58] [INFO] \\U0001f4cb Experiment config: {\\n  \"task\": \"classification\",\\n  \"n_trials\": 2,\\n  \"cv\": 2,\\n  \"learners\": [\\n    \"random_forest\"\\n  ],\\n  \"data_shape\": [\\n    40,\\n    5\\n  ]\\n}\\n[Warning] Trial 1 failed: 'xgboost'\\n[Warning] Trial 2 failed: 'xgboost'\n---------------------------- Captured stderr call -----------------------------\n[I 2025-08-02 18:29:58,373] A new study created in memory with name: no-name-777d006a-bcd8-4df9-9b52-2adeee507682\n_____________________ TestSimpleAPI.test_invalid_learner ______________________\n\nself = <test_simple_api.TestSimpleAPI object at 0x000002751C123650>\n\n    def test_invalid_learner(self):\n        \"\"\"Test that invalid learners raise appropriate errors.\"\"\"\n>       with pytest.raises(ValueError):\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\nE       Failed: DID NOT RAISE <class 'ValueError'>\n\ntests\\test_simple_api.py:210: Failed\n_____________________ TestSimpleAPI.test_default_learners _____________________\n\nself = <test_simple_api.TestSimpleAPI object at 0x000002751C1F4F50>\n\n    def test_default_learners(self):\n        \"\"\"Test that default learners are used when none specified.\"\"\"\n        # Classification defaults\n        automl_clf = SimpleAutoML(task=\"classification\")\n>       assert len(automl_clf.learner_names) > 0\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nE       TypeError: object of type 'NoneType' has no len()\n\ntests\\test_simple_api.py:217: TypeError\n============================== warnings summary ===============================\n..\\..\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pydantic\\_internal\\_config.py:323\n  C:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pydantic\\_internal\\_config.py:323: PydanticDeprecatedSince20: Support for class-based `config` is deprecated, use ConfigDict instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/\n    warnings.warn(DEPRECATION_MESSAGE, DeprecationWarning)\n\n-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html\n=========================== short test summary info ===========================\nFAILED tests/test_simple_api.py::TestSimpleAPI::test_get_learner_info - KeyEr...\nFAILED tests/test_simple_api.py::TestSimpleAPI::test_simple_automl_classification\nFAILED tests/test_simple_api.py::TestSimpleAPI::test_simple_automl_regression\nFAILED tests/test_simple_api.py::TestSimpleAPI::test_single_learner - ValueEr...\nFAILED tests/test_simple_api.py::TestSimpleAPI::test_quick_classification - V...\nFAILED tests/test_simple_api.py::TestSimpleAPI::test_quick_regression - Value...\nFAILED tests/test_simple_api.py::TestSimpleAPI::test_results_summary - ValueE...\nFAILED tests/test_simple_api.py::TestSimpleAPI::test_invalid_learner - Failed...\nFAILED tests/test_simple_api.py::TestSimpleAPI::test_default_learners - TypeE...\n=================== 9 failed, 1 passed, 1 warning in 16.96s ===================\n",
        "stderr": ""
      },
      "test_callbacks.py": {
        "success": false,
        "returncode": 1,
        "stdout": "============================= test session starts =============================\nplatform win32 -- Python 3.13.2, pytest-8.4.1, pluggy-1.6.0 -- C:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python313\\python.exe\ncachedir: .pytest_cache\nrootdir: C:\\Users\\ADMIN\\Desktop\\MLTeammate\nconfigfile: pytest.ini\nplugins: anyio-4.9.0, cov-6.2.1\ncollecting ... collected 27 items\n\ntests/test_callbacks.py::TestBaseCallback::test_base_callback_initialization PASSED [  3%]\ntests/test_callbacks.py::TestBaseCallback::test_base_callback_methods_exist PASSED [  7%]\ntests/test_callbacks.py::TestBaseCallback::test_base_callback_methods_callable PASSED [ 11%]\ntests/test_callbacks.py::TestLoggerCallback::test_logger_callback_initialization PASSED [ 14%]\ntests/test_callbacks.py::TestLoggerCallback::test_logger_callback_with_mlflow PASSED [ 18%]\ntests/test_callbacks.py::TestLoggerCallback::test_logger_callback_with_file PASSED [ 22%]\ntests/test_callbacks.py::TestLoggerCallback::test_logger_callback_level_filtering PASSED [ 25%]\ntests/test_callbacks.py::TestLoggerCallback::test_logger_callback_experiment_lifecycle PASSED [ 29%]\ntests/test_callbacks.py::TestLoggerCallback::test_logger_callback_mlflow_integration PASSED [ 33%]\ntests/test_callbacks.py::TestProgressCallback::test_progress_callback_initialization PASSED [ 37%]\ntests/test_callbacks.py::TestProgressCallback::test_progress_callback_experiment_start PASSED [ 40%]\ntests/test_callbacks.py::TestProgressCallback::test_progress_callback_trial_end PASSED [ 44%]\ntests/test_callbacks.py::TestProgressCallback::test_progress_callback_eta_calculation PASSED [ 48%]\ntests/test_callbacks.py::TestProgressCallback::test_progress_callback_early_stopping_suggestion FAILED [ 51%]\ntests/test_callbacks.py::TestArtifactCallback::test_artifact_callback_initialization PASSED [ 55%]\ntests/test_callbacks.py::TestArtifactCallback::test_artifact_callback_trial_end PASSED [ 59%]\ntests/test_callbacks.py::TestArtifactCallback::test_artifact_callback_save_configs PASSED [ 62%]\ntests/test_callbacks.py::TestArtifactCallback::test_artifact_callback_save_model PASSED [ 66%]\ntests/test_callbacks.py::TestArtifactCallback::test_artifact_callback_experiment_end PASSED [ 70%]\ntests/test_callbacks.py::TestCreateCallbacks::test_create_callbacks_default FAILED [ 74%]\ntests/test_callbacks.py::TestCreateCallbacks::test_create_callbacks_with_logging FAILED [ 77%]\ntests/test_callbacks.py::TestCreateCallbacks::test_create_callbacks_with_progress FAILED [ 81%]\ntests/test_callbacks.py::TestCreateCallbacks::test_create_callbacks_with_artifacts FAILED [ 85%]\ntests/test_callbacks.py::TestCreateCallbacks::test_create_callbacks_all_enabled FAILED [ 88%]\ntests/test_callbacks.py::TestCreateCallbacks::test_create_callbacks_with_kwargs FAILED [ 92%]\ntests/test_callbacks.py::TestCallbackIntegration::test_callback_lifecycle FAILED [ 96%]\ntests/test_callbacks.py::TestCallbackIntegration::test_mlflow_integration_lifecycle PASSED [100%]\n\n================================== FAILURES ===================================\n____ TestProgressCallback.test_progress_callback_early_stopping_suggestion ____\n\nself = <test_callbacks.TestProgressCallback object at 0x00000224ABC75FD0>\n\n    def test_progress_callback_early_stopping_suggestion(self):\n        \"\"\"Test early stopping suggestion.\"\"\"\n        callback = ProgressCallback(total_trials=10, patience=2)\n        callback.start_time = time.time() - 10\n        callback.completed_trials = 5\n        callback.trials_since_improvement = 2\n    \n        with patch('builtins.print') as mock_print:\n>           callback.on_trial_end(\"trial1\", {\"param\": 1}, 0.7, False)\n\ntests\\test_callbacks.py:207: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nself = <ml_teammate.automl.callbacks.ProgressCallback object at 0x00000224CE10F820>\ntrial_id = 'trial1', config = {'param': 1}, score = 0.7, is_best = False\n\n    def on_trial_end(self, trial_id: str, config: dict, score: float, is_best: bool) -> None:\n        \"\"\"Update progress and show status.\"\"\"\n        self.completed_trials += 1\n    \n        if is_best:\n            self.best_score = score\n            self.trials_since_improvement = 0\n        else:\n            self.trials_since_improvement += 1\n    \n        # Calculate progress\n        progress = (self.completed_trials / self.total_trials) * 100\n        eta = self._calculate_eta()\n    \n        # Show progress bar\n        bar_length = 30\n        filled_length = int(bar_length * self.completed_trials // self.total_trials)\n        bar = '\\u2588' * filled_length + '-' * (bar_length - filled_length)\n    \n        print(f\"\\\\r\\U0001f4ca Progress: [{bar}] {progress:.1f}% ({self.completed_trials}/{self.total_trials}) \"\n>             f\"| Best: {self.best_score:.4f} | ETA: {eta}\", end='')\n                        ^^^^^^^^^^^^^^^^^^^^^\nE       TypeError: unsupported format string passed to NoneType.__format__\n\nml_teammate\\automl\\callbacks.py:228: TypeError\n______________ TestCreateCallbacks.test_create_callbacks_default ______________\n\nself = <test_callbacks.TestCreateCallbacks object at 0x00000224ABBCE0D0>\n\n    def test_create_callbacks_default(self):\n        \"\"\"Test create_callbacks with default parameters.\"\"\"\n        callbacks = create_callbacks()\n>       assert len(callbacks) == 0  # No total_trials provided\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^\nE       assert 2 == 0\nE        +  where 2 = len([<ml_teammate.automl.callbacks.LoggerCallback object at 0x00000224CE1B4490>, <ml_teammate.automl.callbacks.ArtifactCallback object at 0x00000224CE17B650>])\n\ntests\\test_callbacks.py:291: AssertionError\n___________ TestCreateCallbacks.test_create_callbacks_with_logging ____________\n\nself = <test_callbacks.TestCreateCallbacks object at 0x00000224ABBCEAD0>\n\n    def test_create_callbacks_with_logging(self):\n        \"\"\"Test create_callbacks with logging enabled.\"\"\"\n        callbacks = create_callbacks(logging=True)\n>       assert len(callbacks) == 1\nE       assert 2 == 1\nE        +  where 2 = len([<ml_teammate.automl.callbacks.LoggerCallback object at 0x00000224CE3B8C00>, <ml_teammate.automl.callbacks.ArtifactCallback object at 0x00000224CE3B8D10>])\n\ntests\\test_callbacks.py:296: AssertionError\n___________ TestCreateCallbacks.test_create_callbacks_with_progress ___________\n\nself = <test_callbacks.TestCreateCallbacks object at 0x00000224ABC0CD60>\n\n    def test_create_callbacks_with_progress(self):\n        \"\"\"Test create_callbacks with progress enabled.\"\"\"\n>       callbacks = create_callbacks(progress=True, total_trials=10)\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\ntests\\test_callbacks.py:301: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nlogging = True, progress = True, artifacts = True, kwargs = {'total_trials': 10}\ncallbacks = [<ml_teammate.automl.callbacks.LoggerCallback object at 0x00000224CE441550>]\n\n    def create_callbacks(logging: bool = True,\n                        progress: bool = True,\n                        artifacts: bool = True,\n                        **kwargs) -> list:\n        \"\"\"\n        Create a list of callbacks with common configurations.\n    \n        Args:\n            logging: Whether to include LoggerCallback\n            progress: Whether to include ProgressCallback\n            artifacts: Whether to include ArtifactCallback\n            **kwargs: Additional arguments for callbacks\n    \n        Returns:\n            List of configured callbacks\n        \"\"\"\n        callbacks = []\n    \n        if logging:\n            callbacks.append(LoggerCallback(**kwargs.get('logging', {})))\n    \n        if progress and 'total_trials' in kwargs:\n>           callbacks.append(ProgressCallback(**kwargs.get('progress', {})))\n                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nE           TypeError: ProgressCallback.__init__() missing 1 required positional argument: 'total_trials'\n\nml_teammate\\automl\\callbacks.py:355: TypeError\n__________ TestCreateCallbacks.test_create_callbacks_with_artifacts ___________\n\nself = <test_callbacks.TestCreateCallbacks object at 0x00000224ABC0DCD0>\n\n    def test_create_callbacks_with_artifacts(self):\n        \"\"\"Test create_callbacks with artifacts enabled.\"\"\"\n        callbacks = create_callbacks(artifacts=True)\n>       assert len(callbacks) == 1\nE       assert 2 == 1\nE        +  where 2 = len([<ml_teammate.automl.callbacks.LoggerCallback object at 0x00000224CE441E50>, <ml_teammate.automl.callbacks.ArtifactCallback object at 0x00000224CE3B98C0>])\n\ntests\\test_callbacks.py:308: AssertionError\n____________ TestCreateCallbacks.test_create_callbacks_all_enabled ____________\n\nself = <test_callbacks.TestCreateCallbacks object at 0x00000224ABC75130>\n\n    def test_create_callbacks_all_enabled(self):\n        \"\"\"Test create_callbacks with all callbacks enabled.\"\"\"\n>       callbacks = create_callbacks(\n            logging=True,\n            progress=True,\n            artifacts=True,\n            total_trials=10\n        )\n\ntests\\test_callbacks.py:313: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nlogging = True, progress = True, artifacts = True, kwargs = {'total_trials': 10}\ncallbacks = [<ml_teammate.automl.callbacks.LoggerCallback object at 0x00000224CE49CB90>]\n\n    def create_callbacks(logging: bool = True,\n                        progress: bool = True,\n                        artifacts: bool = True,\n                        **kwargs) -> list:\n        \"\"\"\n        Create a list of callbacks with common configurations.\n    \n        Args:\n            logging: Whether to include LoggerCallback\n            progress: Whether to include ProgressCallback\n            artifacts: Whether to include ArtifactCallback\n            **kwargs: Additional arguments for callbacks\n    \n        Returns:\n            List of configured callbacks\n        \"\"\"\n        callbacks = []\n    \n        if logging:\n            callbacks.append(LoggerCallback(**kwargs.get('logging', {})))\n    \n        if progress and 'total_trials' in kwargs:\n>           callbacks.append(ProgressCallback(**kwargs.get('progress', {})))\n                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nE           TypeError: ProgressCallback.__init__() missing 1 required positional argument: 'total_trials'\n\nml_teammate\\automl\\callbacks.py:355: TypeError\n____________ TestCreateCallbacks.test_create_callbacks_with_kwargs ____________\n\nself = <test_callbacks.TestCreateCallbacks object at 0x00000224ABBDA360>\n\n    def test_create_callbacks_with_kwargs(self):\n        \"\"\"Test create_callbacks with custom parameters.\"\"\"\n>       callbacks = create_callbacks(\n            logging=True,\n            progress=True,\n            total_trials=10,\n            logging_kwargs={\"log_level\": \"DEBUG\"},\n            progress_kwargs={\"patience\": 3}\n        )\n\ntests\\test_callbacks.py:326: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nlogging = True, progress = True, artifacts = True\nkwargs = {'logging_kwargs': {'log_level': 'DEBUG'}, 'progress_kwargs': {'patience': 3}, 'total_trials': 10}\ncallbacks = [<ml_teammate.automl.callbacks.LoggerCallback object at 0x00000224CE49D130>]\n\n    def create_callbacks(logging: bool = True,\n                        progress: bool = True,\n                        artifacts: bool = True,\n                        **kwargs) -> list:\n        \"\"\"\n        Create a list of callbacks with common configurations.\n    \n        Args:\n            logging: Whether to include LoggerCallback\n            progress: Whether to include ProgressCallback\n            artifacts: Whether to include ArtifactCallback\n            **kwargs: Additional arguments for callbacks\n    \n        Returns:\n            List of configured callbacks\n        \"\"\"\n        callbacks = []\n    \n        if logging:\n            callbacks.append(LoggerCallback(**kwargs.get('logging', {})))\n    \n        if progress and 'total_trials' in kwargs:\n>           callbacks.append(ProgressCallback(**kwargs.get('progress', {})))\n                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nE           TypeError: ProgressCallback.__init__() missing 1 required positional argument: 'total_trials'\n\nml_teammate\\automl\\callbacks.py:355: TypeError\n_______________ TestCallbackIntegration.test_callback_lifecycle _______________\n\nself = <test_callbacks.TestCallbackIntegration object at 0x00000224ABBCEC10>\n\n    def test_callback_lifecycle(self):\n        \"\"\"Test complete callback lifecycle.\"\"\"\n        callbacks = [\n            LoggerCallback(),\n            ProgressCallback(total_trials=3),\n            ArtifactCallback()\n        ]\n    \n        # Test experiment start\n        for callback in callbacks:\n            callback.on_experiment_start({\"n_trials\": 3})\n    \n        # Test trial lifecycle\n        for i in range(3):\n            trial_id = f\"trial_{i}\"\n            config = {\"param\": i}\n            score = 0.7 + i * 0.1\n            is_best = (i == 2)  # Last trial is best\n    \n            for callback in callbacks:\n                callback.on_trial_start(trial_id, config)\n>               callback.on_trial_end(trial_id, config, score, is_best)\n\ntests\\test_callbacks.py:362: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nself = <ml_teammate.automl.callbacks.ProgressCallback object at 0x00000224CE442450>\ntrial_id = 'trial_0', config = {'param': 0}, score = 0.7, is_best = False\n\n    def on_trial_end(self, trial_id: str, config: dict, score: float, is_best: bool) -> None:\n        \"\"\"Update progress and show status.\"\"\"\n        self.completed_trials += 1\n    \n        if is_best:\n            self.best_score = score\n            self.trials_since_improvement = 0\n        else:\n            self.trials_since_improvement += 1\n    \n        # Calculate progress\n        progress = (self.completed_trials / self.total_trials) * 100\n        eta = self._calculate_eta()\n    \n        # Show progress bar\n        bar_length = 30\n        filled_length = int(bar_length * self.completed_trials // self.total_trials)\n        bar = '\\u2588' * filled_length + '-' * (bar_length - filled_length)\n    \n        print(f\"\\\\r\\U0001f4ca Progress: [{bar}] {progress:.1f}% ({self.completed_trials}/{self.total_trials}) \"\n>             f\"| Best: {self.best_score:.4f} | ETA: {eta}\", end='')\n                        ^^^^^^^^^^^^^^^^^^^^^\nE       TypeError: unsupported format string passed to NoneType.__format__\n\nml_teammate\\automl\\callbacks.py:228: TypeError\n---------------------------- Captured stdout call -----------------------------\n[2025-08-02 18:30:13] [INFO] \\U0001f680 Starting MLTeammate experiment: mlteammate_experiment\\n[2025-08-02 18:30:13] [INFO] \\U0001f4cb Experiment config: {\\n  \"n_trials\": 3\\n}\\n\\U0001f3af Progress tracking enabled for 3 trials\\n[2025-08-02 18:30:13] [INFO] \\U0001f52c Starting trial 1 (ID: trial_0...)\\n[2025-08-02 18:30:13] [INFO] \\u2699\\ufe0f  Config: {\\n  \"param\": 0\\n}\\n[2025-08-02 18:30:13] [INFO] \\U0001f4ca Trial 1 completed - Score: 0.7000\\n[2025-08-02 18:30:13] [INFO] \\u23f1\\ufe0f  Duration: 0.00s\n============================== warnings summary ===============================\ntests/test_callbacks.py::TestLoggerCallback::test_logger_callback_with_mlflow\n  C:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pydantic\\_internal\\_config.py:323: PydanticDeprecatedSince20: Support for class-based `config` is deprecated, use ConfigDict instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/\n    warnings.warn(DEPRECATION_MESSAGE, DeprecationWarning)\n\n-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html\n=========================== short test summary info ===========================\nFAILED tests/test_callbacks.py::TestProgressCallback::test_progress_callback_early_stopping_suggestion\nFAILED tests/test_callbacks.py::TestCreateCallbacks::test_create_callbacks_default\nFAILED tests/test_callbacks.py::TestCreateCallbacks::test_create_callbacks_with_logging\nFAILED tests/test_callbacks.py::TestCreateCallbacks::test_create_callbacks_with_progress\nFAILED tests/test_callbacks.py::TestCreateCallbacks::test_create_callbacks_with_artifacts\nFAILED tests/test_callbacks.py::TestCreateCallbacks::test_create_callbacks_all_enabled\nFAILED tests/test_callbacks.py::TestCreateCallbacks::test_create_callbacks_with_kwargs\nFAILED tests/test_callbacks.py::TestCallbackIntegration::test_callback_lifecycle\n================== 8 failed, 19 passed, 1 warning in 10.48s ===================\n",
        "stderr": ""
      }
    },
    "total_tests": 0,
    "passed_tests": 0,
    "failed_tests": 0,
    "success": true
  },
  "integration_tests": {
    "result": {
      "success": false,
      "returncode": 2,
      "stdout": "============================= test session starts =============================\nplatform win32 -- Python 3.13.2, pytest-8.4.1, pluggy-1.6.0 -- C:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python313\\python.exe\ncachedir: .pytest_cache\nrootdir: C:\\Users\\ADMIN\\Desktop\\MLTeammate\nconfigfile: pytest.ini\nplugins: anyio-4.9.0, cov-6.2.1\ncollecting ... collected 0 items / 1 error\n\n=================================== ERRORS ====================================\n____________ ERROR collecting tests/test_tutorials_integration.py _____________\nImportError while importing test module 'C:\\Users\\ADMIN\\Desktop\\MLTeammate\\tests\\test_tutorials_integration.py'.\nHint: make sure your test modules/packages have valid Python names.\nTraceback:\n..\\..\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\importlib\\__init__.py:88: in import_module\n    return _bootstrap._gcd_import(name[level:], package, level)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\ntests\\test_tutorials_integration.py:18: in <module>\n...",
      "stderr": ""
    },
    "total_tests": 0,
    "passed_tests": 0,
    "failed_tests": 0,
    "success": true
  },
  "performance_tests": {
    "result": {
      "success": false,
      "returncode": 1,
      "stdout": "============================= test session starts =============================\nplatform win32 -- Python 3.13.2, pytest-8.4.1, pluggy-1.6.0 -- C:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python313\\python.exe\ncachedir: .pytest_cache\nrootdir: C:\\Users\\ADMIN\\Desktop\\MLTeammate\nconfigfile: pytest.ini\nplugins: anyio-4.9.0, cov-6.2.1\ncollecting ... collected 15 items\n\ntests/test_performance_benchmarks.py::TestDatasetSizePerformance::test_small_dataset_performance FAILED [  6%]\ntests/test_performance_benchmarks.py::TestDatasetSizePerformance::test_medium_dataset_performance FAILED [ 13%]\ntests/test_performance_benchmarks.py::TestDatasetSizePerformance::test_large_dataset_performance FAILED [ 20%]\ntests/test_performance_benchmarks.py::TestDatasetSizePerformance::test_dataset_size_scalability FAILED [ 26%]\ntests/test_performance_benchmarks.py::TestTrialCountPerformance::test_few_trials_performance FAILED [ 33%]\ntests/test_performance_benchmarks.py::TestTrialCountPerformance::test_many_trials_p...",
      "stderr": ""
    },
    "success": false
  },
  "error_handling_tests": {
    "result": {
      "success": false,
      "returncode": 1,
      "stdout": "============================= test session starts =============================\nplatform win32 -- Python 3.13.2, pytest-8.4.1, pluggy-1.6.0 -- C:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python313\\python.exe\ncachedir: .pytest_cache\nrootdir: C:\\Users\\ADMIN\\Desktop\\MLTeammate\nconfigfile: pytest.ini\nplugins: anyio-4.9.0, cov-6.2.1\ncollecting ... collected 35 items\n\ntests/test_error_handling.py::TestDataValidationErrors::test_empty_data FAILED [  2%]\ntests/test_error_handling.py::TestDataValidationErrors::test_single_sample_data FAILED [  5%]\ntests/test_error_handling.py::TestDataValidationErrors::test_mismatched_dimensions FAILED [  8%]\ntests/test_error_handling.py::TestDataValidationErrors::test_invalid_data_types FAILED [ 11%]\ntests/test_error_handling.py::TestDataValidationErrors::test_nan_values FAILED [ 14%]\ntests/test_error_handling.py::TestDataValidationErrors::test_infinite_values FAILED [ 17%]\ntests/test_error_handling.py::TestConfigurationErrors::test_invalid_task_type FAILED [...",
      "stderr": ""
    },
    "total_tests": 0,
    "passed_tests": 0,
    "failed_tests": 0,
    "success": true
  },
  "coverage": {
    "success": false,
    "error": "Coverage run failed"
  },
  "summary": {
    "duration_seconds": 151.5500783920288,
    "duration_formatted": "151.55s",
    "total_tests": 0,
    "total_passed": 0,
    "total_failed": 0,
    "success_rate_percent": 0,
    "all_tests_passed": true,
    "unit_tests_passed": true,
    "integration_tests_passed": true,
    "performance_tests_passed": false,
    "error_handling_tests_passed": true,
    "coverage_passed": false
  }
}